#+TITLE: sparcur developer guide
#+AUTHOR: Tom Gillespie
#+OPTIONS: num:nil ^:nil h:7
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+STARTUP: showall

# [[file:developer-guide.pdf]]
# [[file:developer-guide.html]]

* Demos
** Remote only connection
If you have [[(def_babf)][~sparcur.simple~]] this is the simplest way to get a remote
only connection to Blackfynn.
#+begin_src python :results drawer :epilogue "import pprint; return pprint.pformat(datasets)"
from sparcur.config import auth
from sparcur.simple.utils import backend_blackfynn
project_id = auth.get('blackfynn-organization')
BlackfynnRemote = backend_blackfynn(project_id)
root = BlackfynnRemote(project_id)
datasets = list(root.children)
#+end_src

Otherwise you have to do what backend_blackfynn does for you
#+begin_src python :results drawer :epilogue "import pprint; return pprint.pformat(datasets)"
from sparcur.paths import BlackfynnCache, Path
from sparcur.config import auth
from sparcur.backends import BlackfynnRemote
project_id = auth.get('blackfynn-organization')
BlackfynnRemote = BlackfynnRemote._new(Path, BlackfynnCache)
BlackfynnRemote.init(project_id)
root = BlackfynnRemote(BlackfynnRemote.root)
datasets = list(root.children)
#+end_src
** Validate a dataset
You can run this example block and it will validate the [[file:../resources/DatasetTemplate][DatasetTemplate]].

# #+header: :epilogue "import pprint; return pprint.pformat(data.keys())"
#+header: :epilogue "import pprint; return pprint.pformat(data)"
#+begin_src python :results drawer :exports both :cache yes :tangle ./broken.py
from sparcur import pipelines as pipes
from sparcur.paths import Path as basePath


def makeValidator(dataset_path):

    class Path(basePath):
        """ Workaround absense of cache. """

        # TODO likely will also need to rebind the cache class as well

        @property
        def dataset_relative_path(self, __drp=dataset_path):
            return self.relative_path_from(self.__class__(__drp))

    Path._bind_flavours()

    # XXX monkey patch TODO sigh FIXME DatasetStructure calls Path directly inside
    basePath.dataset_relative_path = Path.dataset_relative_path

    dataset_path = Path(dataset_path)

    class context:
        path = dataset_path.resolve()
        id = path.id
        uri_api = path.as_uri()
        uri_human = path.as_uri()

    class lifters:
        id = context.id
        folder_name = context.path.name
        uri_api = context.uri_api
        uri_human = context.uri_human
        timestamp_export_start = None

    return pipes.SDSPipeline(dataset_path, lifters, context)


path = basePath('../resources/DatasetTemplate')
pipeline = makeValidator(path)
data = pipeline.data
#+end_src

#+RESULTS[99f2d5eba83acdff6934d4fdfe64da4170348550]:
:results:
:end:

** Load Python IR
#+begin_src python
from sparcur.utils import path_ir
ir = path_ir('curation-export.json')
#+end_src

#+begin_src python
def main():
    from sparcur.reports import Report
    from sparcur.utils import path_ir
    from pyontutils.core import OntResIri
    ori = OntResIri('https://cassava.ucsd.edu/sparc/preview/exports/curation-export.ttl')
    graph = ori.graph

    ir = path_ir('/tmp/curation-export-test.json')

    rows = Report._hubmap_terms(graph, ir)

    anat = [r for r in rows if r[1].prefix in ('UBERON', 'FMA', 'ILX')]
    all_r = expand_label_curie(rows)
    ana_r = expand_label_curie(anat)
    return all_r, ana_r


if __name__ == '__main__':
    return main()
#+end_src
* Extending implementation
** Adding a new xml type
#+begin_src python
def new_xml_format(path):
    from sparcur.extract import xml
    ex = xml.XmlSource(path)
    top_tag = ex.e.getroot().tag
#+end_src
* Workflow
** All datasets
*** _*Retrieve*_
**** Overview                                                        :ignore:
The dependency DAG is as follows.
# NOTE the workflow for generating these diagrams takes multiple steps
# first write the graph in racket, where we can use dashes in names
# conver to dot and add clusters as needed
#+name: graph-retrieve-all
#+header: :wrap "src dot :file ./images/graph-retrieve-all.png :cmdline -Kdot -Tpng :exports results :cache yes"
#+begin_src racket :lang racket/base :exports none :noweb no-export :cache yes
<<racket-graph-helper>>
(define g (dag-notation
           fetch-all -> fetch-metadata-files -> pull -> sparse-materialize -> fetch-remote-metadata
           fetch-all ->      fetch-files     -> pull -> clone
           fetch-all -> fetch-remote-metadata
           fetch-all -> fetch-annotations
))

(graphviz g)
#+end_src

#+RESULTS[5b1ab6330a12cfe55439af47a6bd717498fc6c7d]: graph-retrieve-all
#+begin_src dot :file ./images/graph-retrieve-all.png :cmdline -Kdot -Tpng :exports results :cache yes
digraph G {
	node0 [label="fetch-metadata-files"];
	node1 [label="clone"];
	node2 [label="fetch-all"];
	node3 [label="fetch-remote-metadata"];
	node4 [label="pull"];
	node5 [label="fetch-annotations"];
	node6 [label="sparse-materialize"];
	node7 [label="fetch-files"];
	subgraph U {
		edge [dir=none];
	}

	subgraph cluster_F {
        color=none;
        node2;
	}

	subgraph cluster_D {
        label="Dataset";
        color=green;
		node0 -> node4;
		node2 -> node7;
		node2 -> node0;
		node2 -> node3;
		node4 -> node1;
		node4 -> node6;
		node6 -> node3;
		node7 -> node4;
	}

	subgraph cluster_P {
        label="Protcur";
        color=purple;
		node2 -> node5;
    }
}
#+end_src

#+RESULTS[8febbedbaf66631abc1d1c9ed53915698665c236]:
[[file:./images/graph-retrieve-all.png]]
**** Bash implementation                                             :ignore:
# TODO we need some way to snapshot and deliver the contents of
# --cache-path (usually ~/.cache/sparcur/) so that pipelines can be
# rerun in a way that is deterministic, this is primarily an issue for
# the blackfynn remote metadata that is updated at every run the quick
# and dirty fix is to set SPARCUR_CACHE_PATH=${PARENT_PATH}/cache and
# then symlink in everything except the blackfynn-meta folder but the
# right thing to do is probably to add a configuration option for the
# location of each cache folder

# FIXME this is really an env file not a bin file ...
#+name: pipeline-functions-sparc-get-all-remote-data
#+begin_src bash -r -l "\([[:space:]]*#[[:space:]]*(ref:%s)\|[[:space:]]*(ref:%s)\)$" :tangle ../bin/pipeline-functions.sh :mkdirp yes
function sparc-get-all-remote-data () {
    # NOTE not quite all the remote data, the google sheets
    # don't have caching functionality yet

    # parse args
    local POSITIONAL=()
    while [[ $# -gt 0 ]]
    do
    key="$1"
    case $key in # (ref:(((((((sigh)
        --project-id)         local PROJECT_ID="${2}"; shift; shift ;;
        --symlink-objects-to) local SYMLINK_OBJECTS_TO="${2}"; shift; shift ;;
        --log-path)           local LOG_PATH="${2}"; shift; shift ;;
        --parent-path)        local PARENT_PATH="${2}"; shift; shift ;;
        --only-filesystem)    local ONLY_FILESYSTEM="ONLY_FS"; shift ;;
        -h|--help)            echo "${HELP}"; return ;;
        ,*)                    POSITIONAL+=("$1"); shift ;;
    esac
    done

    # Why, you might be asking, are we declaring a local project path here without assignment?
    # Well. Let me tell you. Because local is a command with an exist status. So it _always_
    # returns zero. So if you need to check the output of the command running in a subshell
    # that you are assigning to a local variable _ALWAYS_ set local separately first.
    # Yes, shellcheck does warn about this. See also https://superuser.com/a/1103711
    local PROJECT_PATH
    local UTC_OFFSET_START
    local TIME_START_NO_OFFSET

    # gnu coreutils gdate needed for osx support/freebsd
    # gdate on darwin only has millisecond resolution?
    # this also won't work on freebsd without gnu coreutils
    iso8601millis="+%FT%T,%6N"  # FIXME do we _really_ need millis!? yemaybe? concurrent startups?
    utcoffset="+%:z"
    # I really hope the utc offset doesn't change between start & end
    # but laptops and airplains do exist, so it could
    # also how utterly annoying that the date separator and the
    # negative utc offset share the same symbol ... talk about
    # an annoying design flaw that is going to haunt humanity
    # with double the number of calls to date for # ... as
    # long as anoyone is writing code to deal with time
    TIME_START_NO_OFFSET=$(date ${iso8601millis} || gdate ${iso8601millis})
    UTC_OFFSET_START=$(date ${utcoffset} || gdate ${utcoffset})
    local TIME_START="${TIME_START_NO_OFFSET}${UTC_OFFSET_START}"  # XXX unused

    local TIME_START_NO_OFFSET_FS_OK=${TIME_START_NO_OFFSET//:/}
    local UTC_OFFSET_START_FS_OK=${UTC_OFFSET_START//:/}
    local TIME_START_FRIENDLY=${TIME_START_NO_OFFSET_FS_OK}${UTC_OFFSET_START_FS_OK}
    # So. iso8601 guidance on what to do about subsecond time and the utc offset in the compact
    # representation is not entirely clear, however I _think_ that %FT%T%H%M%S,%6N%z is ok but
    # the -/+ must stay between the timezone and the rest, so we will have to grab tz by itself
    local TIME_START_SAFE=${TIME_START_NO_OFFSET_FS_OK//-/}${UTC_OFFSET_START_FS_OK}  # XXX unused
    if [[ -z "${PARENT_PATH}" ]]; then
        mv "$(mktemp --directory sparcur-all-XXXXXX)" "${TIME_START_FRIENDLY}" || \
            { CODE=$?; echo 'mv failed'; return $CODE; }
    fi
    local PARENT_PATH=${PARENT_PATH:-$TIME_START_FRIENDLY}
    local LOG_PATH=${LOG_PATH:-"${PARENT_PATH}/logs"}

    #local LOG_PATH=$(python -c "from sparcur.config import auth; print(auth.get_path('log-path'))")
    local PROJECT_ID=${PROJECT_ID:-$(python -c "from sparcur.config import auth; print(auth.get('blackfynn-organization'))")}

    local maybe_slot=()
    if [[ -n "${SYMLINK_OBJECTS_TO}" ]]; then
        # MUST use arrays to capture optional arguments like this otherwise
        # arg values with spaces in them will destroy your sanity
        maybe_slot+=(--symlink-objects-to "${SYMLINK_OBJECTS_TO}")
    fi

    echo "${PARENT_PATH}"  # needed to be able to follow logs

    if [ ! -d "${LOG_PATH}" ]; then
        mkdir "${LOG_PATH}" || { CODE=$?; echo 'mkdir of ${LOG_PATH} failed'; return $CODE; }
    fi

    if [[ -z "${ONLY_FILESYSTEM}" ]]; then
        # fetch annotations (ref:bash-pipeline-fetch-annotations)
        echo "Fetching annotations metadata"
        python -m sparcur.simple.fetch_annotations > "${LOG_PATH}/fetch-annotations.log" 2>&1 &
        local pids_final[0]=$!

        # fetch remote metadata (ref:bash-pipeline-fetch-remote-metadata-all)
        # if this fails with 503 errors, check the
        # blackfynn-backoff-factor config variable
        echo "Fetching remote metadata"
        python -m sparcur.simple.fetch_remote_metadata_all \
            --project-id "${PROJECT_ID}" \
            > "${LOG_PATH}/fetch-remote-metadata.log" 2>&1 &
        local pids[0]=$!
    fi

    local FAIL=0

    # clone aka fetch top level

    # we do not background this assignment because it runs quickly
    # and everything that follows depends on it finishing, plus we
    # need it to finish to set the PROJECT_PATH variable here
    echo python -m sparcur.simple.clone --project-id "${PROJECT_ID}" --parent-path "${PARENT_PATH}" "${maybe_slot[@]}"
    echo "Cloning top level"
    set -o pipestatus
    PROJECT_PATH=$(python -m sparcur.simple.clone \
                          --project-id "${PROJECT_ID}" \
                          --parent-path "${PARENT_PATH}" \
                          "${maybe_slot[@]}" \
                          2>&1 | tee "${LOG_PATH}/clone.log" | tail -n 1) || {
        # TODO tee the output when verbose is passed
        CODE=$?;
        tail -n 100 "${LOG_PATH}/clone.log";
        echo "Clone failed! The last 100 lines of ${LOG_PATH}/clone.log are listed above.";
        apids=( "${pids[@]}" "${pids_final[@]}" );
        for pid in "${apids[@]}"; do
            kill $pid;
        done;
        set +o pipefail
        return $CODE;
    }
    set +o pipefail

    # explicit export of the current project path for pipelines
    # ideally we wouldn't need this, and when this pipeline
    # finished the export pipeline would kick off, or the export
    # pipeline would search for ... an existing project path ...
    # by ... oh right, looking for an environment variable or
    # checksing some other persistent state ... so this is the one
    # unless some controlling process sets it top down from the start
    # but we can't assume that
    export SPARCUR_PROJECT_PATH="${PROJECT_PATH}"

    for pid in "${pids[@]}"; do
        wait $pid || { FAIL=$((FAIL+1)); echo "${pid} failed!"; }
    done
    if [[ $FAIL -ne 0 || -z "${PROJECT_PATH}" ]]; then
        echo "${FAIL} commands failed. Cannot continue."
        echo "${PROJECT_PATH}"
        return 1
    fi

    # pull aka fetch file system metadata
    echo "Fetching file system metadata"
    echo python -m sparcur.simple.pull --project-path "${PROJECT_PATH}"
    python -m sparcur.simple.pull \
           --project-path "${PROJECT_PATH}" \
           > "${LOG_PATH}/pull.log" 2>&1 || {
        CODE=$?;
        tail -n 100 "${LOG_PATH}/pull.log";
        echo "Pull failed! The last 100 lines of ${LOG_PATH}/pull.log are listed above.";
        echo "${PROJECT_PATH}";
        return $CODE; }

    # fetch metadata files
    echo "Fetching metadata files"
    # have to pass project path as a position argument here so that it
    # does not try to pull aka fetch the file system metadata again
    echo python -m sparcur.simple.fetch_metadata_files --project-path "${PROJECT_PATH}"
    python -m sparcur.simple.fetch_metadata_files \
           --project-path "${PROJECT_PATH}" \
           > "${LOG_PATH}/fetch-metadata-files.log" 2>&1 &

    pids_final[1]=$!

    # fetch files
    echo "Fetching files"
    # XXX at some point this will probably also depend on the manifests
    # so we don't fetch everything with a matching extension
    # TODO derive --extension from manifests or all it to be passed in
    echo python -m sparcur.simple.fetch_metadata_files --project-path "${PROJECT_PATH}" --extension xml

    # FIXME fetch_files fails silently here :/
    python -m sparcur.simple.fetch_files \
           --project-path "${PROJECT_PATH}" \
           --extension xml \
           > "${LOG_PATH}/fetch-files.log" 2>&1 &

    pids_final[2]=$!

    local FAIL=0
    for pid in "${pids_final[@]}"; do
        wait $pid || { FAIL=$((FAIL+1)); echo "${pid} failed!"; }
    done

    # FIXME HACK
    #find -type f -size 0 -exec getfattr -d {} \;
    #find -type f -size 0 -exec spc fetch --limit=-1 {} \;

    if [[ $FAIL -ne 0 ]]; then
        echo "${FAIL} commands failed. Cannot continue."
        echo "${PROJECT_PATH}"
        return 1
    fi
}
#+end_src
*** _*Validate*_
**** Overview                                                        :ignore:
This is the graph of the existing approach more or less as implemented
by ~spc export~.

A slightly more sane version is being implemented as part of
=sparcur.simple= which will sandbox the network dependencies.

# runs both but I'm fairly cerain that it fails to update the second code block
# #+name: graph-validate-run-both
# #+begin_src elisp :var one=graph-validate-all() two=graph-validate-all-dot() :results none
# #+end_src

#+name: graph-validate-all
#+header: :wrap "src dot :file ./images/graph-validate-all.png :cmdline -Kdot -Tpng :exports results :cache yes"
#+begin_src racket :lang racket/base :exports none :noweb no-export :cache yes
<<racket-graph-helper>>
(define g (dag-notation
           ; I had description listed depending on dataset-structure
           ; but that is really an implementation detail

           pipeline-end -> pipeline-extras -> sparc-ds -> pipeline-start -> description -> fetch-all
                                                          pipeline-start -> dataset-structure -> fetch-all
                                                          pipeline-start -> dataset-metadata -> fetch-all

                                              ; note that this is the idealized flow
                                              ; the actual flow is through pipeline-start
                                              sparc-ds -> submission -> description
                                                          submission -> fetch-all
                                              sparc-ds -> subjects -> description
                                                          subjects -> fetch-all
                                              sparc-ds -> samples -> description
                                                          samples -> fetch-all
                                              sparc-ds -> manifest -> description
                                                          manifest -> fetch-all

                           pipeline-extras -> submission-normalized -> submission
                           pipeline-extras -> pipeline-files-xml -> cache -> fetch-all ; -> fetch-files
                           pipeline-extras -> contributors -> affiliations #;lifters -> affiliations-sheet -> sheets -> network
                                              contributors -> member #;state -> blackfynn-api -> network
                                              contributors -> description
                           pipeline-extras -> meta-extra -> dataset-doi -> pipeline-remote-metadata -> cache ; -> fetch-remote-metadata
                                              meta-extra -> dataset-remote-readme -> pipeline-remote-metadata
                                              meta-extra -> dataset-remote-status -> pipeline-remote-metadata

                                              meta-extra -> organ-term #;lifters -> organs-sheet -> sheets
                                              meta-extra -> modality #;lifters -> organs-sheet
                                              meta-extra -> techniques #;lifters -> organs-sheet
                                              meta-extra -> protocol-uris #;lifters -> organs-sheet
                                              meta-extra -> award-manual #;lifters -> organs-sheet
                                              meta-extra -> award-organ #;lifters -> submission-normalized
                                                            award-organ -> scraped-award-organ
                           pipeline-extras -> pipeline-extras-updates -> identifier-resolution -> network
                           pipeline-extras -> pipeline-protcur -> cache ; -> fetch-annotations
           ??? -> overview-sheet -> sheets))

;; subgraphs
(define lifters '(affiliations organ-term modality techniques protocol-uris award-manual award-organ))
(define state '(member))
(define network '(network blackfynn-api sheets affiliation-sheet organs-sheet overview-sheet))

(define-vertex-property g vertex-id #:init $id)  ; doesn't work to get the graphviz node numbering

(define-vertex-property g in-lifters?)
(for-each (λ (v) (in-lifters?-set! v #t)) lifters)

(define-vertex-property g in-state?)
(for-each (λ (v) (in-state?-set! v #t)) state)

(define-vertex-property g in-network?)
(for-each (λ (v) (in-network?-set! v #t)) network)

(graphviz g)
#+end_src

#+name: graph-validate-all-dot
#+RESULTS[16bcd2566c9bc6aca9c4c547144fe50c5a542558]: graph-validate-all
#+begin_src dot :file ./images/graph-validate-all.png :cmdline -Kdot -Tpng :exports results :cache yes
digraph G {
	node0 [label="description"];
	node1 [label="modality"];
	node2 [label="dataset-doi"];
	node3 [label="blackfynn-api"];
	node4 [label="dataset-metadata"];
	node5 [label="samples"];
	node6 [label="subjects"];
	node7 [label="award-manual"];
	node8 [label="submission-normalized"];
	node9 [label="organs-sheet"];
	node10 [label="scraped-award-organ"];
	node11 [label="member"];
	node12 [label="sheets"];
	node13 [label="award-organ"];
	node14 [label="network"];
	node15 [label="submission"];
	node16 [label="fetch-all"];
	node17 [label="manifest"];
	node18 [label="techniques"];
	node19 [label="overview-sheet"];
	node20 [label="pipeline-extras"];
	node21 [label="pipeline-end"];
	node22 [label="pipeline-start"];
	node23 [label="protocol-uris"];
	node24 [label="affiliations"];
	node25 [label="affiliations-sheet"];
	node26 [label="contributors"];
	node27 [label="organ-term"];
	node28 [label="meta-extra"];
	node29 [label="dataset-structure"];
	node30 [label="sparc-ds"];
	node31 [label="???"];
	subgraph U {
		edge [dir=none];
	}
	subgraph cluster_M {
		label="Metadata Files";
		color=green;
        node0;
        node5;
        node6;
        node15;
        node17;
	}
	subgraph cluster_L {
		label="Lifters (bad design)";
		color=red;
        node1;
        node7;
        node13;
        node18;
        node24;
        node23;
        node27;
	}
	subgraph D {
		node0 -> node16;
		node1 -> node9;
		node2 -> node3;
		node3 -> node14;
		node4 -> node16;
		node5 -> node0;
		node5 -> node16;
		node6 -> node0;
		node6 -> node16;
		node7 -> node9;
		node8 -> node15;
		node9 -> node12;
		node11 -> node3;
		node12 -> node14;
		node13 -> node10;
		node13 -> node8;
		node15 -> node0;
		node15 -> node16;
		node17 -> node0;
		node17 -> node16;
		node18 -> node9;
		node19 -> node12;
		node20 -> node30;
		node20 -> node28;
		node20 -> node26;
		node20 -> node8;
		node21 -> node20;
		node22 -> node0;
		node22 -> node29;
		node22 -> node4;
		node23 -> node9;
		node24 -> node25;
		node25 -> node12;
		node26 -> node0;
		node26 -> node11;
		node26 -> node24;
		node27 -> node9;
		node28 -> node1;
		node28 -> node2;
		node28 -> node18;
		node28 -> node13;
		node28 -> node23;
		node28 -> node7;
		node28 -> node27;
		node29 -> node16;
		node30 -> node17;
		node30 -> node5;
		node30 -> node15;
		node30 -> node22;
		node30 -> node6;
		node31 -> node19;
	}
}
#+end_src

#+RESULTS[20008f92af2cbbe5a5aa89221885829ea3bd0f11]: graph-validate-all-dot
[[file:./images/graph-validate-all.png]]
**** ??? implementation                                              :ignore:
*** _*Export*_
**** Overview                                                        :ignore:
In the current implementation validation and export are conflated.
This is bad, and will be changed.

=spc export= must only be run after =sparc-get-all-remote-data=,
otherwise there will be network sandbox violations.

For the record there are multiple way invoke =spc export=.
#+begin_src bash :eval never
# pushd to the project location
pushd "${PROJECT_PATH:-SPARCUR_PROJECT_PATH}"
spc export
popd
# pass the project location as a positional argument
spc export "${PROJECT_PATH:-SPARCUR_PROJECT_PATH}"
# pass the project location as an option
spc export --project-path "${PROJECT_PATH:-SPARCUR_PROJECT_PATH}"
#+end_src

At the moment =sparc-export-all= is just a wrapper around =spc export=.
This will change as we move to a single dataset export model. There
will then likely be a function that checks for datasets that have
changed since last export, updates only those and then collects the
outputs.
**** Bash implementation                                             :ignore:
#+name: pipeline-functions-sparc-export-all

#+begin_src bash -r -l "\([[:space:]]*#[[:space:]]*(ref:%s)\|[[:space:]]*(ref:%s)\)$" :tangle ../bin/pipeline-functions.sh
function sparc-export-all () {
    # parse args
    local POSITIONAL=()
    while [[ $# -gt 0 ]]
    do
    key="$1"
    case $key in # (ref:(((sigh)
        --project-path) local PROJECT_PATH="${2}"; shift; shift ;;
        -h|--help)      echo "${HELP}"; return ;;
        ,*)              POSITIONAL+=("$1"); shift ;;
    esac
    done

    local PROJECT_PATH="${PROJECT_PATH:-$SPARCUR_PROJECT_PATH}"
    spc export --project-path "${PROJECT_PATH}"
}
#+end_src
** Single dataset
*** _*Retrieve*_
**** Overview                                                        :ignore:
**** Bash implementation                                             :ignore:
#+begin_src bash
function sparcur-retrieve () {
    "First pass retrieve all remote data for a single dataset."
    local DATASET_ID=$1
    local DATASET_PATH
    DATASET_PATH=$(python -m sparcur.simple.retrieve --dataset-id ${DATASET_ID})
    # FIXME error handling
    echo $DATASET_PATH
}
#+end_src
*** _*Validate*_
**** Extract
#+begin_src bash
python -m sparcur.simple.extract "${DATASET_PATH}"
#+end_src
**** Retrieve 2
*** _*Export*_
** Future correct single dataset workflow
Network access should only be possible during the retrieve phase.

The validate step may happen during extract and transform as well
since structure or data content issues may be easier to detect during
certain phases. Ideally this would not be the case, but practically it
will take more work than necessary given our use cases.

We have to be careful to separate basic validation of the structure of
the dataset data from the validation that the identifiers provided in
that structure point to values known to their respective remotes.

For example we need to be able to say =you are missing a protocol reference=
at a separate point in time from saying =the remote(s) we asked had no record=
=of the protocol reference you provided=.
*** Hypothes.is
This is pulled in bulk independently in a different workflow but it is
probably worth checking to see if we need to run it again whenever we
run a dataset.
*** Structure metadata
**** Retrieve
This is ref:clone.py, ref:fetch_remote_metadata_all.py, and ref:pull.py.
**** Extract
**** Transform
**** Validate
We can't do this right now because the current dataset template cannot
be statically validated. Only some of it can be validated when we have
the data from the subjects and samples sheets. In this future pipeline
the type prefixes will be required so that the structure can be
statically verified.
*** Dataset metadata
Run this if the structure metadata is in a state where we can proceed
(i.e. that there is a dataset_description file).
**** Retrieve
This is ref:fetch_metadata_files.py.
**** Extract
**** Transform
**** Validate
*** File metadata
**** Retrieve
This is ref:fetch_files.py.
Depends on the manifest files and the dataset structure.
**** Extract
**** Transform
**** Validate
This is local validation, not remote networked validation.
*** Identifiers and mapping
**** Retrieve
This is the retrieval/dereferencing of identifier metadata.

It must happen after the file metadata step has been completed so that
e.g. identifiers used in MBF segmentation files can be validated. In
this step in particular validation and retrieval are essentially the
same step. If there is an error during retrieval then it must produce
a validation error.
*** Protocols
Checking and/or retrieving these depends on 3 things.
The protocols.io group, the hypothesis group, and the dataset metadata.
*** Protc
Needs hypothesis and protocols.
*** Export
**** Convert
**** Serialize
Probably also includes load in some cases e.g. for the file level
metadata that will be attached to package-id file-id pairs.
# TODO issue with dumping packages into mongo is that we will
# have to flag packages and collections as deleted
** Protocols
#+name: graph-protocols
#+header: :wrap "src dot :file ./images/graph-protocols.png :cmdline -Kdot -Tpng :exports results :cache yes"
#+begin_src racket :lang racket/base :exports none :noweb no-export :cache yes
<<racket-graph-helper>>
(define g (dag-notation
           export-protcur -> pipeline-protcur -> cache-annotations -> fetch-annotations
                             pipeline-protcur -> fetch-protocols-io -> network
                             ; FIXME fetching the protocols.io metadata is a major network sandbox violation
                             ; ideally we can untangle this, but it is difficult, also issues with how we
                             ; are caching put this at risk of going stale
))

(graphviz g)
#+end_src

#+RESULTS[aaeaed353b6b51181c18cdb722696d821a27f63f]: graph-protocols
#+begin_src dot :file ./images/graph-protocols.png :cmdline -Kdot -Tpng :exports results :cache yes
digraph G {
	node0 [label="fetch-protocols-io"];
	node1 [label="pipeline-protcur"];
	node2 [label="network"];
	node3 [label="cache-annotations"];
	node4 [label="export-protcur"];
	node5 [label="fetch-annotations"];
	subgraph U {
		edge [dir=none];
	}
	subgraph D {
		node0 -> node2;
		node1 -> node3;
		node1 -> node0;
		node3 -> node5;
		node4 -> node1;
	}
}
#+end_src

#+RESULTS[e419d8438b4609bab73327984f217d394a78f995]:
[[file:./images/graph-protocols.png]]
** Release
Since we are moving to run individual datasets the aggregate release
process is decouple, and mediated via git:36a749b5c321cdb81ba81f9d35e050ceb8479976
*** Testing
**** Code
**** Data
*** Reporting
After a release
* Internal Structure
:PROPERTIES:
:header-args:python: :comments link :exports code :mkdirp yes
:header-args:python+: :shebang "#!/usr/bin/env python3"
:END:
** Pipelines
Easier to read, harder to debug. The python paradox.
*** _*Retrieve*_
**** _Protocols_
Cache annotations.
See [[(bash-pipeline-fetch-annotations)]] for usage.
#+name: fetch_annotations.py
#+begin_src python :tangle ../sparcur/simple/fetch_annotations.py :mkdirp yes
from pathlib import Path
from hyputils import hypothesis as hyp
from sparcur.config import auth


def from_group_name_fetch_annotations(group_name):
    """ pull hypothesis annotations from remote to local """
    group_id = auth.user_config.secrets('hypothesis', 'group', group_name)
    cache_file = Path(hyp.group_to_memfile(group_id + 'sparcur'))
    get_annos = hyp.Memoizer(cache_file, group=group_id)
    get_annos.api_token = auth.get('hypothesis-api-key')  # FIXME ?
    annos = get_annos()
    return cache_file  # needed for next phase, annos are not


def main(hypothesis_group_name=None, **kwargs):
    if hypothesis_group_name is None:
        hypothesis_group_name = 'sparc-curation'

    from_group_name_fetch_annotations(hypothesis_group_name)


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)
#+end_src
**** _Datasets_
***** Clone
This is an example of how to clone the top level of a project.
See ref:utils.py for a good way to instantiate =RemotePath=.
#+name: clone.py
#+begin_src python :tangle ../sparcur/simple/clone.py
from pathlib import Path


# clone top level
def from_path_id_and_backend_project_top_level(parent_path,
                                               project_id,
                                               RemotePath,
                                               symlink_objects_to=None,):
    """ given the enclosing path to clone to, the project_id, and a fully
        configured (with Local and Cache) backend remote path, anchor the
        project pointed to by project_id along with the first level of children """

    project_path = _from_path_id_remote_project(parent_path,
                                                project_id,
                                                RemotePath,
                                                symlink_objects_to)
    return _from_project_path_top_level(project_path)


def from_path_project_backend_id_dataset(parent_path,
                                         project_id,
                                         dataset_id,
                                         RemotePath,
                                         symlink_objects_to=None,):
    project_path = _from_path_id_remote_project(parent_path,
                                                project_id,
                                                RemotePath,
                                                symlink_objects_to)
    return _from_project_path_id_dataset(project_path, dataset_id)


def _from_path_id_remote_project(parent_path, project_id, RemotePath, symlink_objects_to):
    RemotePath.init(project_id)  # calling init is required to bind RemotePath._api
    anchor = RemotePath.smartAnchor(parent_path)
    anchor.local_data_dir_init(symlink_objects_to=symlink_objects_to)
    project_path = anchor.local
    return project_path


def _from_project_path_top_level(project_path):
    """ given a project path with existing cached metadata
        pull the top level children

        WARNING: be VERY careful about using this because it
        does not gurantee that rmeta is available to mark
        sparse datasets. It may be the case that the process
        will fail if the rmeta is missing, or it may not. Until
        we are clear on the behavior this warning will stay
        in place. """
    # this is a separate function in case the previous step fails
    # which is also why it is hidden, it makes too many assuptions
    # to be used by itself

    anchor = project_path.cache
    list(anchor.children)  # this fetchs data from the remote path to the local path
    return project_path  # returned instead of anchor & children because it is needed by next phase


def _from_project_path_id_dataset(project_path, dataset_id):
    anchor = project_path.cache
    remote = anchor._remote_class(dataset_id)
    cache = anchor / remote
    return cache.local


def main(parent_path=None,
         project_id=None,
         parent_parent_path=Path.cwd(),
         project_id_auth_var='blackfynn-organization',  # FIXME move default to clifun
         symlink_objects_to=None,
         id=None,
         dataset_id=None,
         ,**kwargs):
    """ clone a project into a random subfolder of the current folder
        or specify the parent path to clone into """

    from sparcur.simple.utils import backend_blackfynn

    if parent_path is None:
        breakpoint()  # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX FIXME
        import tempfile
        parent_path = Path(tempfile.mkdtemp(dir=parent_parent_path))

    if project_id is None:
        from sparcur.config import auth
        project_id = auth.get(project_id_auth_var)

    RemotePath = backend_blackfynn()

    if id and dataset_id:
        # FIXME doesn't check for existing so if the name changes we get duped folders
        # this issue possibly upstream in retrieve, clone just clones whatever you tell
        # it to clone, but maybe it should check the existing metadata and fail or warn?
        dataset_path = from_path_project_backend_id_dataset(
            parent_path,
            project_id,
            id,  # FIXME multiple datasets
            RemotePath,
            symlink_objects_to,)

        return dataset_path

    project_path = from_path_id_and_backend_project_top_level(
        parent_path,
        project_id,
        RemotePath,
        symlink_objects_to,)

    return project_path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
***** Remote metadata
Remote metadata must be retrieved prior to the first pull
in order to ensure that large datasets can be marked as
sparse datasets before they are pulled.
****** From id
Remote metadata can be retrieved using only a project_id. However,
for all retrieval after the first pull it is usually more effective
to retrieve it at the same time as fetching metadata files since it
runs in parallel per dataset.

See [[(bash-pipeline-fetch-remote-metadata-all)]] for usage.
#+name: fetch_remote_metadata_all.py
#+begin_src python :tangle ../sparcur/simple/fetch_remote_metadata_all.py
from joblib import Parallel, delayed
from sparcur.backends import BlackfynnDatasetData
from sparcur.simple.utils import backend_blackfynn


def from_id_fetch_remote_metadata(id, project_id=None, n_jobs=12):
    """ given an dataset id fetch its associated dataset metadata """
    if id.startswith('N:organization'):
        RemotePath = backend_blackfynn()
        project = RemotePath(id)
        prepared = [BlackfynnDatasetData(r) for r in project.children]
        if n_jobs <= 1:
            [p() for p in prepared]
        else:
            Parallel(n_jobs=n_jobs)(delayed(p)() for p in prepared)
    elif id.startswith('N:dataset:'):  # TODO support dataset: ?
        RemotePath = backend_blackfynn(project_id)
        dataset = RemotePath(id)
        bdd = BlackfynnDatasetData(dataset)
        bdd()
    else:
        raise NotImplementedError(id)


def main(id=None,
         project_id=None,
         project_id_auth_var='blackfynn-organization',  # FIXME move to clifun
         n_jobs=12,
         **kwargs):

    if project_id is None:
        from sparcur.config import auth
        project_id = auth.get(project_id_auth_var)

    if id is None:
        id = project_id

    from_id_fetch_remote_metadata(id,
                                  project_id=project_id,
                                  n_jobs=n_jobs,)


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)  # nothing to print or do after
#+end_src
****** From path
:PROPERTIES:
:CUSTOM_ID: fetch-remote-metadata
:END:
The implementation of =sparcur.backends.BlackfynnDatasetData= supports the ability
to retrieve metadata directly from the remote without the need for an intervening
local path. However this functionality is obscured here because we want to derive
a consistent view of the data from the file system snapshot.
# NOTE check where this is used, currently it is only used in simple.pull.main
# which will trigger only if pull is called without any other arguments and not
# in an existing project which is not a good way to do any of this
#+name: fetch_remote_metadata.py
#+begin_src python :tangle ../sparcur/simple/fetch_remote_metadata.py
from joblib import Parallel, delayed
from sparcur.paths import Path
from sparcur.backends import BlackfynnDatasetData


def _from_project_path_fetch_remote_metadata(project_path, n_jobs=12, cached_ok=False):
    if n_jobs <= 1:
        prepared = [BlackfynnDatasetData(dataset_path.cache)
                    for dataset_path in project_path.children]
        [bdd() for bdd in prepared if not (cached_ok and bdd.cache_path.exists())]
    else:
        fetch = lambda bdd: bdd() if not (cached_ok and bdd.cache_path.exists()) else None
        fetch_path = (lambda path: fetch(BlackfynnDatasetData(path.cache)))
        Parallel(n_jobs=n_jobs)(delayed(fetch_path)(dataset_path)
                 for dataset_path in project_path.children)


# fetch remote metadata
def from_path_fetch_remote_metadata(path, n_jobs=12, cached_ok=False):
    """ Given a path fetch remote metadata associated with that path. """

    cache = path.cache
    if cache.is_organization():
        _from_project_path_fetch_remote_metadata(path, n_jobs=n_jobs, cached_ok=cached_ok)
    else:  # dataset_path
        # TODO more granular rather than roll up to dataset if inside?
        bdd = BlackfynnDatasetData(cache)
        if not (cached_ok and bdd.cache_path.exists()):
            bdd()


def main(path=Path.cwd(), n_jobs=12, rmeta_cached_ok=False, **kwargs):
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.clone import main as clone
        path = clone(path=path, n_jobs=n_jobs, **kwargs)
        # NOTE path is passed along here, but kwargs is expected to contain
        # parent_path or parent_parent_path and project_id note that if that
        # happens then the path returned from clone will change accordingly

    from_path_fetch_remote_metadata(path, n_jobs=n_jobs, cached_ok=rmeta_cached_ok)
    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)  # we probably don't print here?
#+end_src
***** Pull
Pull a single dataset or pull all datasets or clone and pull all datasets.
#+name: pull.py
#+begin_src python :tangle ../sparcur/simple/pull.py
from joblib import Parallel, delayed
from sparcur.paths import Path
from sparcur.utils import GetTimeNow


# pull dataset
def from_path_dataset_file_structure(path, time_now=None, exclude_uploaded=False):
    """ pull the file structure and file system metadata for a single dataset
        right now only works from a dataset path """

    if time_now is None:
        time_now = GetTimeNow()

    path._pull_dataset(time_now, exclude_uploaded)


# pull all in parallel
def from_path_dataset_file_structure_all(project_path,
                                         ,*args,
                                         paths=None,
                                         time_now=None,
                                         n_jobs=12,
                                         exclude_uploaded=False):
    """ pull all of the file structure and file system metadata for a project
        paths is a keyword argument that accepts a list/tuple of the subset of
        paths that should be pulled """

    if time_now is None:
        time_now = GetTimeNow()

    project_path.pull(
        paths=paths,
        time_now=time_now,  # TODO
        debug=False,  # TODO
        n_jobs=n_jobs,
        log_level='DEBUG' if False else 'INFO',  # TODO
        Parallel=Parallel,
        delayed=delayed,
        exclude_uploaded=exclude_uploaded,)


# mark datasets as sparse 
def sparse_materialize(path, sparse_limit:int=None):
    """ given a path mark it as sparse if it is a dataset and
        beyond the sparse limit """

    cache = path.cache
    if cache.is_organization():
        # don't iterate over cache children because that pulls remote data
        for child in path.children:
            sparse_materialize(child, sparse_limit=sparse_limit)
    else:
        cache._sparse_materialize(sparse_limit=sparse_limit)


def main(path=Path.cwd(),
         time_now=None,
         sparse_limit:int=None,
         n_jobs=12,
         exclude_uploaded=False,
         ,**kwargs):
    project_path = None  # path could be None so can't find_cache_root here
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.fetch_remote_metadata import main as remote_metadata
        project_path = remote_metadata(path=path, **kwargs)  # transitively calls clone
    else:
        project_path = path.find_cache_root()
        if path != project_path:
            # dataset_path case
            sparse_materialize(path, sparse_limit=sparse_limit)
            from_path_dataset_file_structure(path, time_now=time_now, exclude_uploaded=exclude_uploaded)
            if path == Path.cwd():
                print('NOTE: you probably need to run `pushd ~/ && popd` '
                      'to get a sane view of the filesystem if you ran this'
                      'from within a dataset folder')
            return path

    if not list(project_path.children):
        raise FileNotFoundError(f'{project_path} has no children.')
        # somehow clone failed
        # WARNING if rmeta failed you may get weirdness  # FIXME
        from sparcur.simple.clone import _from_project_path_top_level
        _from_project_path_top_level(project_path)

    sparse_materialize(project_path,
                       sparse_limit=sparse_limit)
    from_path_dataset_file_structure_all(project_path,
                                         time_now=time_now,
                                         n_jobs=n_jobs,
                                         exclude_uploaded=exclude_uploaded)
    return project_path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
***** Fetch
#+caption: NOTE this block is unused!
#+begin_src python :tangle ../sparcur/simple/fetch.py :exports none
from sparcur.simple.fetch_metadata_files import main as files
from sparcur.simple.fetch_remote_metadata import main as rmeta


def main(path=Path.cwd(), **kwargs):
    if path is None or not path.find_cache_root() in (path, *path.parents):
        from sparcur.simple.pull import main as pull
        path = pull(path=path, n_jobs=n_jobs, **kwargs)

    # FIXME these can be run in parallel
    # python is not its own best glue code ...
    rmeta(path=path)
    files(path=path)
    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
****** Metadata files
# ugh I gave myself the name in a loop variable colliding with
# name at higher level of indentation still in a loop bug, so
# totally will overwrite the name and cause madness to ensue
#+name: fetch_metadata_files.py
#+begin_src python :tangle ../sparcur/simple/fetch_metadata_files.py
from itertools import chain
from sparcur import exceptions as exc
from sparcur.utils import log, logd
from sparcur.paths import Path
from sparcur.datasets import DatasetStructure
from sparcur.simple.utils import fetch_paths_parallel, rglob

# fetch metadata files
fetch_prefixes = (
    ('dataset_description', 'glob'),
    ('subjects',            'glob'),
    ('samples',             'glob'),
    ('submission',          'glob'),
    ('manifest',           'rglob'),  # XXX NOTE the rglob here
)


def _from_path_fetch_metadata_files_simple(path, fetch=True):
    """ transitive yield paths to all metadata files, fetch them from
        the remote if fetch == True """
    for glob_prefix, glob_type in fetch_prefixes:
        if glob_type == 'rglob':
            gp0 = glob_prefix[0]
            pattern = f'[{gp0.upper()}{gp0}]{glob_prefix[1:]}*'
            yield from rglob(path, pattern)
            continue
        ds = DatasetStructure(path)
        for path_to_metadata in ds._abstracted_paths(glob_prefix,
                                                     glob_type=glob_type,
                                                     fetch=fetch):  # FIXME fetch here is broken
            yield path_to_metadata


def _from_path_fetch_metadata_files_parallel(path, n_jobs=12):
    """ Fetch all metadata files within the current path in parallel. """
    paths_to_fetch = _from_path_fetch_metadata_files_simple(path, fetch=False)
    try:
        first = next(paths_to_fetch)
        paths_to_fetch = chain((first,), paths_to_fetch)
    except StopIteration:
        log.warning('No paths to fetch, did you pull the file system metadata?')
        return

    # FIXME passing in a generator here fundamentally limits the whole fetching
    # process to a single thread because the generator is stuck feeding from a
    # single process, IF you materialize the paths first then the parallel fetch
    # can actually run in parallel, but bugs/errors encountered late in collecting
    # the paths will force all previous work to be redone
    # XXX as a result of this we use the posix find command to implement rglob
    # in a way that is orders of magnitude faster
    paths_to_fetch = list(paths_to_fetch)
    fetch_paths_parallel(paths_to_fetch, n_jobs=n_jobs)


def from_path_fetch_metadata_files(path, n_jobs=12):
    """ fetch metadata files located within a path """
    #if n_jobs <= 1:
        #gen = _from_path_fetch_metadata_files_simple(path)
        # FIXME broken ??? somehow abstracted paths doesn't fetch when
        # we run in directly, or somehow fetch_paths_parallel does something
        # different
        #paths = list(gen)
    #else:
    _from_path_fetch_metadata_files_parallel(path, n_jobs=n_jobs)


def main(path=Path.cwd(), n_jobs=12, **kwargs):
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.pull import main as pull
        path = pull(path=path, n_jobs=n_jobs, **kwargs)

    from_path_fetch_metadata_files(path, n_jobs=n_jobs)
    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
****** File level metadata extraction
Fetch files by extension.
#+name: fetch_files.py
#+begin_src python :tangle ../sparcur/simple/fetch_files.py
import os
from sparcur.paths import Path
from sparcur.simple.utils import fetch_paths_parallel


def _datasets_with_extension(path, extension):
    """ Hack around the absurd slowness of python's rglob """

    # TODO query multiple extensions with -o at the same time
    command = fr"""for d in */; do
    find "$d" \( -type l -o -type f \) -name '*.{extension}' \
    -exec getfattr -n user.bf.id --only-values "$d" \; -printf '\n' -quit ;
done"""

    with path:
        with os.popen(command) as p:
            string = p.read()

    has_extension = string.split('\n')
    datasets = [p for p in path.children if p.cache_id in has_extension]
    return datasets


def _from_path_fetch_files_simple(path, filter_function, fetch=True):
    files = filter_function(path)
    if fetch:
        [f.fetch(size_limit_mb=None) for f in files if not f.exists()]
        #Async(rate=5)(deferred(f.fetch)(size_limit_mb=None)
                      #for f in files if not f.exists())

    return files


def _from_path_fetch_files_parallel(path, filter_function, n_jobs=12):
    paths_to_fetch = _from_path_fetch_files_simple(path, filter_function, fetch=False)
    fetch_paths_parallel(paths_to_fetch, n_jobs=n_jobs)


def filter_extensions(extensions):
    """ return a function that selects files in a path by extension """
    def filter_function(path):
        cache = path.cache
        if cache.is_organization():
            paths = set()
            for ext in extensions:
                ds = _datasets_with_extension(path, ext)
                paths.update(ds)

        else:  # dataset_path
            paths = path,

        files = [matching  # FIXME stream ?
                for path in paths
                for ext in extensions
                for matching in path.rglob(f'*.{ext}')]
        return files

    return filter_function


def filter_manifests(dataset_blob):
    """ return a function that selects certain files listed in manifest records """
    # FIXME this needs a way to handle organization level?
    # NOTE this filter is used during the second fetch phase after the inital
    # metadata has been ingested to the point where it can be use to guide further fetches
    # TODO this is going to require the implementation of partial fetching I think
    # TODO preprocessing here?
    def filter_function(path):
        # TODO check that the path and the dataset blob match
        cache = path.cache
        if cache.id != dataset_blob['id']:
            msg = f'Blob is not for this path! {dataset_blob["id"]} != {cache.id}'
            raise ValueError(msg)

        files = []  # TODO get_files_for_secondary_fetch(dataset_blob)
        return files

    return filter_function


def from_path_fetch_files(path, filter_function, n_jobs=12):
    if n_jobs <= 1:
        _from_path_fetch_files_simple(path, filter_function)
    else:
        _from_path_fetch_files_parallel(path, filter_function, n_jobs=n_jobs)


def main(path=Path.cwd(), n_jobs=12, extensions=('xml',), **kwargs):
    #breakpoint()  # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.pull import main as pull
        path = pull(path=path, n_jobs=n_jobs, **kwargs)

    filter_function = filter_extensions(extensions)

    from_path_fetch_files(path, filter_function, n_jobs=n_jobs)
    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)
#+end_src
****** Second fetch
Once the initial pass over the dataset is complete extract the list of
additional files that need to be retrieved and fetch them.
# TODO partial fetch of headers for MBF embedded metadata
#+name: fetch_secondary.py
#+begin_src python :tangle ../sparcur/simple/fetch_secondary.py
from sparcur.paths import Path
from sparcur.simple.fetch_files import from_path_fetch_files, filter_manifests


def from_blob_fetch_files(dataset_blob, path=None):
    # should the blob contain a reference to the path
    # it was derived from?
    filter_function = filter_manifests(dataset_blob)
    from_path_fetch_files(path, filter_function, n_jobs=n_jobs)


def main(path=Path.cwd(), n_jobs=12, **kwargs):
    #breakpoint()  # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    # if not dataset_blob: get_blob vs path blob pairs?
    # starting from a partial blob means that this probably
    # should not kick off from the file system, but we know
    # that we will want to be able to kick it off from the
    # file system ... maybe the intermediate blobs can encode
    # the prov of where the file system reference they were
    # derived from lives ?
    dataset_blob = get_blob(path.cache_id)  # FIXME TODO
    from_blob_fetch_files(dataset_blob, path)


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)
#+end_src
****** unused :noexport:
#+begin_src python
from_id_remote_metadata = lambda id: ds.BlackfynnDatasetData(id)()
compose = lambda f, g: (lambda *x: f(g(*x)))
#from_path_remote_metadata = compose(lambda id: from_id_remote_metadata(id),
                                    #lambda path: path.cache.id)
#+end_src
***** Retrieve
Putting it all together into a single command.

The behavior of retrieve works exactly as it does for clone the
difference is that it runs for just a single dataset and the
parent_path is made to be the dataset_id uuid if you are running a
single dataset pipeline you will still need the project folder
structure for logs and jobs etc.  you can also still run all datasets
together off of a single SPARC Consoritum folder, in which case all
you need to do is pass the communal parent_path

#+name: retrieve.py
#+begin_src python :tangle ../sparcur/simple/retrieve.py
from pathlib import Path
from sparcur.simple.clone import main as clone
from sparcur.simple.fetch_remote_metadata_all import main as remote_metadata
from sparcur.simple.pull import main as pull
from sparcur.simple.fetch_metadata_files import main as fetch_metadata_files


def main(id=None,
         dataset_id=tuple(),
         parent_path=None,
         parent_parent_path=Path.cwd(),
         path=None,  # keep path out of kwargs
         ,**kwargs):
    # FIXME parent_path and project_id seem like they probably need to
    # be passed here, it would be nice if project_path could just be
    # the current folder and if the xattrs are missing for the
    # project_id then ... it is somehow inject from somewhere else?
    # this doesn't really work, because that would mean that we would
    # have to carry the project id around in the xattr metadata for
    # all dataset folders, which might not be the worst thing, but
    # definitely redundant
    if id is None:
        raise TypeError('id is a required argument!')

    if parent_path is None:
        N, thing_type, uuid = id.split(':')
        parent_path = parent_parent_path / uuid
        parent_path.mkdir(exist_ok=True)

    # XXX for now we do these steps in order here
    # rather than trusting that calling simple.pull.main will do
    # the right thing if there is no path ... it should but probably
    # doesn't right now due to assumptions about paths existing

    # remote metadata from path (could do from id too?)
    remote_metadata(id=id, **kwargs)  # could run parallel to clone, easier in bash
    # clone single without organization parent somehow seems likely broken?
    path = clone(id=id,
                 dataset_id=dataset_id,
                 parent_path=parent_path,
                 parent_parent_path=parent_parent_path,
                 ,**kwargs)
    # pull single
    pull(path=path, **kwargs)
    # fetch metadata files
    fetch_metadata_files(path=path, **kwargs)

    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
*** _*Validate*_
**** _Protocols_
**** _Datasets_
:PROPERTIES:
:header-args: :shebang "#!/usr/bin/env python3"
:END:
***** Extract
#+name: dataset-extract
#+begin_src python :tangle ../sparcur/simple/extract.py
from sparcur import datasets as dat
from sparcur import pipelines as pipes
from sparcur import exceptions as exc
from sparcur.utils import log, logd
from sparcur.paths import Path
from sparcur.backends import BlackfynnDatasetData
from sparcur.simple.utils import combinate, multiple, early_failure, DataWrapper
from sparcur.simple.fetch_metadata_files import fetch_prefixes


class ManifestFiles(DataWrapper):
    """ wrapper for manifest files. """


def merge_manifests(vs):
    """ Merge individual manifest records into the same list """
    # FIXME massive hack :/
    k = 'manifest_file'
    # FIXME errors key ... ? is it allowed up there? it shouldn't be ...
    # FIXME {'content': m}
    return ManifestFiles([m for v in vs for m in v.data[k]])


def object_from_find_path(glob_prefix, object_from_path_function, glob_type='glob', onfail=None):
    """ Return a function that will find files that start with glob_prefix"""
    # FIXME should be in utils but depends on fetch_prefixes
    if glob_prefix not in dict(fetch_prefixes):
        raise ValueError('glob_prefix not in fetch_prefixes! '
                         f'{glob_prefix!r} not in {fetch_prefixes}')
    def func(path, *args, **kwargs):
        ds = dat.DatasetStructure(path)
        rpath = None
        for rpath in ds._abstracted_paths(glob_prefix, sandbox=True):
            yield object_from_path_function(rpath, *args, **kwargs)

        if rpath is None and onfail is not None:
            raise onfail(f'No match for {glob_prefix} in {path.name}')

    return func

# file type -> dataset blob key indirection

_TOP = object()  # SIGH SIGH SIGH always need a escape hatch

otkm = {ThingFilePath.obj:prefix + '_file' for prefix, ThingFilePath
        in dat.DatasetStructure.sections.items()}
otkm[ManifestFiles] = 'manifest_file'
otkm[BlackfynnDatasetData] = 'remote_dataset_metadata'
otkm[type(dat.DatasetStructure())] = 'structure'  # hack around Pathlib type mangling
otkm[type(dat.DatasetMetadata())] = _TOP

# stream of objects -> place in dataset blob

def dataset_raw(*objects, object_type_key_map=otkm):
    data = {}
    log.debug(objects)
    #path_structure, description, subjects, samples, submission, manifests, *rest = objects
    for obj in objects:
        log.debug(obj)
        key = object_type_key_map[type(obj)]
        try:
            if key is not _TOP:
                data.update({key: obj.data})
            else:
                data.update(obj.data)
        except Exception as e:
            # FIXME current things that leak through
            # MalformedHeaderError
            # something in the objects list is a dict
            breakpoint()
            pass

    return data


# path + version -> python object

# TODO how to attach and validate schemas orthogonally in this setting?
# e.g. so that we can write dataset_1_0_0 dataset_1_2_3 etc.

# we capture version as early as possible in the process, yes we
# could also gather all the files and folders and then pass the version
# in as an argument when we validate their structure, but there are
# elements of the locations or names of those files that might depend
# on the template version, therefore we get maximum flexibility by only
# need to look for the dataset description file
def description(path):          return dat.DatasetDescriptionFilePath(path).object
def submission(path, version):  return dat.SubmissionFilePath(path).object_new(version)
def subjects(path, version):    return dat.SubjectsFilePath(path).object_new(version)
def samples(path, version):     return dat.SamplesFilePath(path).object_new(version)
def manifest(path, version):    return dat.ManifestFilePath(path).object_new(version)

# dataset path -> python object

def from_path_remote_metadata(path): return BlackfynnDatasetData(path.cache)
def from_path_local_metadata(path): return dat.DatasetMetadata(path)
from_path_dataset_description = object_from_find_path('dataset_description', description,
                                                      onfail=exc.MissingFileError)

comb_metadata = combinate(
    # dataset description is not included here because it is special
    # see from_path_dataset_raw for details
    from_path_remote_metadata,
    from_path_local_metadata,
)

# dataset path + version -> python object

def from_path_dataset_path_structure(path, version): return dat.DatasetStructure(path)
from_path_subjects   = object_from_find_path('subjects',            subjects)
from_path_samples    = object_from_find_path('samples',             samples)
from_path_submission = object_from_find_path('submission',          submission)
from_path_manifests  = multiple(object_from_find_path('manifest',   manifest,
                                                      'rglob'),
                                merge_manifests)

# combinate all the individual dataset path + version -> data functions

comb_dataset = combinate(
    #from_path_dataset_description,  # must be run prior to combination to get version
    from_path_dataset_path_structure,
    from_path_subjects,
    from_path_samples,
    from_path_submission,
    from_path_manifests,
    #from_path_file_metadata,  # this must wait until 2nd fetch phase
    )

# dataset path -> raw data

def from_path_dataset_raw(dataset_path):
    """ Main entry point for getting dataset metadata from a path. """
    gen  = from_path_dataset_description(dataset_path)
    try:
        ddo = dataset_description_object = next(gen)
    except exc.MissingFileError as e:
        # TODO return a stub with embedded error
        logd.critical(e)
        dataset_blob = dataset_raw(*comb_metadata(dataset_path))
        return early_failure(dataset_path, e, dataset_blob)

    try:
       next(gen)
       # TODO return a stub with embedded error
    except StopIteration:
        pass

    data = ddo.data
    ddod = type('ddod', tuple(), {'data': data})
    dtsv = data['template_schema_version']
    return dataset_raw(ddo, *comb_metadata(dataset_path), *comb_dataset(dataset_path, dtsv))


# unused

def from_path_file_metadata(path, _version):  # FIXME doesn't go in this file probably
    # FIXME this is going to depend on the manifests
    # and a second fetch step where we kind of cheat
    # and prefetch file files we know we will need
    pass


def from_export_path_protocols_io_data(curation_export_json_path): pass
def protocols_io_ids(datasets): pass
def protocols_io_data(protocols_io_ids): pass

def from_group_name_protcur(group_name): pass
def protcur_output(): pass

def summary(datasets, protocols_io_data, protcur_output): pass

def from_path_summary(project_path):
    dataset_path_structure
    summary((
        dataset(
            dataset_path_structure,
            dataset_description,
            subjects,
            samples,
            submission,
            manifests,
            ,*rest
)))


def main(path=Path.cwd(), dataset_id=None, time_now=None, **kwargs):
    # TODO path from dataset_id and retrieve conventions? or just pass path from retrieve final output?
    # TODO parallelize if multiple paths
    # This assumes that all retrieve operations have
    # finished successfully for the current dataset
    cache = path.cache
    if not cache.is_dataset():
        raise TypeError('can only run on a single dataset')

    dataset_raw = from_path_dataset_raw(path)
    return dataset_raw


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)

#+end_src
***** Transform
There is not a clean separation between transformation and validation
because there are multiple transformation and validation steps that
are interleaved.
#+name: dataset-transform
#+begin_src python :tangle ../sparcur/simple/transform.py
from pathlib import Path
from sparcur import schemas as sc
from sparcur import pipelines as pipes
from sparcur.core import DictTransformer as DT


def __apply_step(step, spec, data, **kwargs):
    return step(data, spec, **kwargs)


def popl(data, pops, source_key_optional=False):
    popped = list(DT.pop(data, pops, source_key_optional))
    return data


def simple_add(data, adds):
    pass


def execute_pipeline(pipeline, data):
    for func, *args, kwargs in pipeline:
        # man variable arity is a pain to deal with here
        # where are lambda lists when you need them :/
        # FIXME maybe we can make steps functional instead of mutating?
        if not kwargs:
            kwargs = {}

        func(data, *args, **kwargs)

    return data


def __wd(transformer):  # not needed atm since transformers do in place modification
    def inner(data, *args, **kwargs):
        transformer(data, *args, **kwargs)
        return data


def schema_validate(data, schema, fail=False, pipeline_stage_name=None):
    if isinstance(schema, type):
        # we were passed a class so init it
        # doing it this way makes it easier to
        # use remote schemas that hit the network
        # since the network call doesn't have to
        # happen at the top level but can mutate
        # the class later before we init here
        schema = schema()

    ok, norm_or_error, data = schema.validate(data)
    if not ok:
        if fail:
            logd.error('schema validation has failed and fail=True')
            raise norm_or_error

        if 'errors' not in data:
            data['errors'] = []

        if pipeline_stage_name is None:
            pipeline_stage_name = f'Unknown.checked_by.{schema.__class__.__name__}'

        data['errors'] += norm_or_error.json(pipeline_stage_name)
        # TODO make sure the step is noted even if the schema is the same


simple_moves = (
    [['structure', 'dirs',], ['meta', 'dirs']],  # FIXME not quite right ...
    [['structure', 'files',], ['meta', 'files']],
    [['structure', 'size',], ['meta', 'size']],
    [['remote_dataset_metadata'], ['inputs', 'remote_dataset_metadata']],
    ,*pipes.SDSPipeline.moves[3:]
)

# function, *args, **kwargs
# functions should accept data as the first argument
core_pipeline = (
    # FIXME validation of dataset_raw is not being done right now
    (DT.copy, pipes.SDSPipeline.copies, dict(source_key_optional=True)),
    (DT.move, simple_moves, dict(source_key_optional=True)),
    # TODO clean has custom behavior
    (popl, pipes.SDSPipeline.cleans, dict(source_key_optional=True)),
    (DT.derive, pipes.SDSPipeline.derives, dict(source_key_optional=True)),
    #(DT.add, pipes.SDSPipeline.adds),  # FIXME lifter issues
    (schema_validate, sc.DatasetOutSchema, None),
    # extras (missing)
    # xml files
    # contributors
    # submission
    (DT.copy, pipes.PipelineExtras.copies, dict(source_key_optional=True)),
    # TODO clean has custom behavior
    (DT.update, pipes.PipelineExtras.updates, dict(source_key_optional=True)),
    (DT.derive, pipes.PipelineExtras.derives, dict(source_key_optional=True)),
    #(DT.add, pipes.PipelineExtras.adds),  # TODO and lots of evil custom behavior here
    # TODO filter_failures
    (schema_validate, sc.DatasetOutSchema, None),
    (pipes.PipelineEnd._indexes, None),  # FIXME this is conditional and in adds
    # TODO pipeline end has a bunch of stuff in here
    (schema_validate, sc.PostSchema, dict(fail=True)),
)


def main(path=Path.cwd(), path_dataset_raw=None, dataset_raw=None, **kwargs):
    # FIXME TODO need to follow the behavior of main in extract
    if dataset_raw is None:
        if path_dataset_raw is None:
            cache = path.cache
            if not cache.is_dataset():
                raise TypeError('can only run on a single dataset')
            from sparcur.simple.extract import main as extract
            dataset_raw = extract(path)
        else:
            from sparcur.utils import path_ir
            dataset_raw = path_ir(path_dataset_raw)

    data = execute_pipeline(core_pipeline, dataset_raw)
    breakpoint()


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
**** Network resources
*** _*Export*_
**** _Dataset Path Metadata_
:PROPERTIES:
:header-args: :shebang "#!/usr/bin/env python3"
:END:
In principle this could run as part of the dataset metadata export,
however since it produces separate files and can run at the same time,
it is its own module.

Usage example
#+begin_src bash
find  -maxdepth 1 -type d -not -path '.operations*' -not -path '.' -exec python -m sparcur.simple.path_metadata {} \;
#+end_src

#+name: dataset-path-metadata-extract
#+begin_src python :tangle ../sparcur/simple/path_metadata.py
from pathlib import Path
from sparcur.utils import transitive_paths


def from_path_transitive_metadata(path):
    tml = path._transitive_metadata()
    # FIXME TODO handle sparse cases
    return {'type': 'path-metadata',
            'data': tml,}


def prepare_dataset_export(path, export_base=None):  # FIXME do we need export_base?
    from pyontutils.utils import timeformat_friendly
    from sparcur.paths import Path
    from sparcur.utils import symlink_latest
    from sparcur.config import auth
    export_base = Path(auth.get_path('export-path'))  # FIXME allow alt?
    N, thing_type, uuid = path.cache.id.split(':')
    # we don't use cache._fs_safe_id here because we know the
    # identifier type from the folder structure
    # FIXME dataset metadata export setup basically needs to do all of this first
    # set latest run and then latest complete at the end, but complete is kind of arbitrary
    # from the atomic point of view
    tupdated = path.updated_cache_transitive()  # FIXME this causes us to travers all files twice
    # FIXME somehow tupdated can be None !??!?!
    export_dataset_folder = export_base / 'datasets' / uuid
    export_parent = export_dataset_folder / timeformat_friendly(tupdated)
    if not export_parent.exists():
        export_parent.mkdir(parents=True)

    export_latest_run = export_dataset_folder / 'LATEST_RUN'
    symlink_latest(export_parent, export_latest_run)
    return export_parent


def main(path=Path.cwd(), time_now=None, export_path=None, **kwargs):
    import json
    from socket import gethostname
    import augpathlib as aug
    import sparcur
    from sparcur.core import JEncode
    from sparcur.paths import Path
    from sparcur.utils import loge, GetTimeNow

    if time_now is None:
        time_now = GetTimeNow()
    # TODO path from dataset_id and retrieve conventions? or just pass path from retrieve final output?
    # TODO parallelize if multiple paths
    # This assumes that all retrieve operations have
    # finished successfully for the current dataset
    path = Path(path)  # FIXME even here some paths don't have caches ?!
    cache = path.cache  # XXX this should have errored when Path was applied below !?!?!??! pipe_main wat ???
    if not cache.is_dataset():
        raise TypeError('can only run on a single dataset')

    export_parent = prepare_dataset_export(path)  # FIXME probably don't call this for each export main?

    tm = from_path_transitive_metadata(path)

    tm['prov'] = {'timestamp_export_start': time_now.START_TIMESTAMP,
                  'export_system_identifier': Path.sysid,
                  'export_hostname': gethostname(),
                  'sparcur_version': sparcur.__version__,
                  }
    rp = aug.RepoPath(sparcur.core.__file__)
    if rp.working_dir is not None:
        tm['prov']['sparcur_commit'] = rp.repo.active_branch.commit.hexsha

    export_path = export_parent / 'path-metadata.json'

    with open(export_path, 'wt') as f:
        json.dump(tm, f, indent=2, cls=JEncode)

    # TODO export to some file
    # FIXME TODO validate file formats, which means this also needs to support the remoteless case
    loge.info(f'path metadata exported to {export_path}')
    return tm


if __name__ == '__main__':
    #import pprint
    from sparcur.simple.utils import pipe_main
    pipe_main(main)#, after=pprint.pprint)

#+end_src
** Utility
*** _*Init*_
#+begin_src python :tangle ../sparcur/simple/__init__.py :exports none
#+end_src
*** _*Utils*_
#+name: utils.py
#+begin_src python :tangle ../sparcur/simple/utils.py
"""Common command line options for all sparcur.simple modules
Usage:
    sparcur-simple [options] [<path>...]
    sparcur-simple [options] [--dataset-id=<ID>...]
    sparcur-simple [options] [--extension=<EXT>...] [<path>...]

Options:
    -h --help                       show this

    --hypothesis-group-name=NAME    the hypotheis group name

    --project-id=ID                 the project id
    --dataset-id=<ID>...            one or more datset ids
    --project-id-auth-var=VAR       name of the auth variable holding the project-id

    --project-path=PATH             the project path, will be path if <path>... is empty
    --parent-path=PATH              the parent path where the project will be cloned to
    --parent-parent-path=PATH       parent in which a random tempdir is generated
                                    to be the parent path, don't use this ...
    --extension=<EXT>...            one or more file extensions to fetch

    -j --jobs=N                     number joblib jobs [default: 12]
    --exclude-uploaded              do not pull files from remote marked as uploaded
    --sparse-limit=N                package count that forces a sparse pull [default: 10000]
    --symlink-objects-to=PATH       path to an existing objects directory
"""

import os
from types import GeneratorType
from pyontutils import clifun as clif
from sparcur import exceptions as exc
from sparcur.utils import log, logd
from sparcur.paths import Path, BlackfynnCache
from sparcur.backends import BlackfynnRemote


def backend_blackfynn(project_id=None, Local=Path, Cache=BlackfynnCache):  # (ref:def_babf)
    """return a configured blackfynn backend
        calling this is sufficient to get everything set up correclty

        You must call RemotePath.init(project_id) before using
        RemotePath.  Passing the project_id argument to this function
        will do that for you. It is not required because there are
        cases where the project_id may not be known at the time that
        this function is called. """

    RemotePath = BlackfynnRemote._new(Local, Cache)

    if project_id is not None:
        RemotePath.init(project_id)

    return RemotePath


class Options(clif.Options):

    @property
    def id(self):
        # dataset_id has priority since project_id can occure without a
        # dataset_id, but dataset_id may sometimes come with a project_id
        # in which case the dataset_id needs priority for functions that
        # branch on the most granular identifier type provided
        return (self.dataset_id[0]
                if self.dataset_id else
                (self.project_id
                 if self.project_id else
                 None))

    @property
    def jobs(self):
        return int(self._args['--jobs'])

    n_jobs = jobs  # match internal kwargs conventions

    @property
    def paths(self):
        return [Path(p).expanduser().resolve() for p in self._args['<path>']]

    @property
    def path(self):
        paths = self.paths
        if paths:
            return paths[0]
        elif self.project_path:
            return self.project_path
        else:
            # if no paths were listed default to cwd
            # consistent with how the default kwargs
            # are set on a number of mains
            # this is preferable to allow path=None
            # to be overwritten by the conventions of
            # individual pipeline mains
            return Path.cwd()

    @property
    def project_path(self):
        pp = self._args['--project-path']
        if pp:
            return Path(pp).expanduser().resolve()

    @property
    def extensions(self):
        return self.extension

    @property
    def symlink_objects_to(self):
        slot = self._args['--symlink-objects-to']
        if slot:
            return Path(slot).expanduser()

    @property
    def sparse_limit(self):  # FIXME not being pulled in by asKwargs ??
        return int(self._args['--sparse-limit'])


def pipe_main(main, after=None, argv=None):
    options, args, defaults = Options.setup(__doc__, argv=argv)
    out = main(**options.asKwargs())
    if after:
        after(out)

    return out


def rglob(path, pattern):
    """ Hack around the absurd slowness of python's rglob """

    doig = (hasattr(path, 'cache') and
            path.cache and
            path.cache.cache_ignore)
    exclude = ' '.join([f"-not -path './{p}*'" for p in path.cache.cache_ignore]) if doig else ''
    command = f"""find {exclude} -name {pattern!r}"""

    with path:
        with os.popen(command) as p:
            string = p.read()

    path_strings = string.split('\n')  # XXX posix path names can contain newlines
    paths = [path / s for s in path_strings if s]
    return paths


def fetch_paths_parallel(paths, n_jobs=12):
    def fetch(cache):
        # lambda functions are great right up until you have to handle an
        # error function inside of them ... thanks python for yet another
        # failure to be homogenous >_<
        meta = cache.meta
        try:
            size_mb = meta.size.mb
        except AttributeError as e:
            if meta.errors:
                logd.debug(f'remote errors {meta.errors} for {cache!r}')
                return
            else:
                raise exc.StopTheWorld(cache) from e

        return cache.fetch(size_limit_mb=size_mb + 1)

    def fetch_path(path):
        cache = path.cache
        if cache is None:
            raise exc.NoCachedMetadataError(path)

        # do not return to avoid cost of serialization back to the control process
        fetch(cache)

    if n_jobs <= 1:
        [fetch_path(path) for path in paths]
    else:
        from joblib import Parallel, delayed
        Parallel(n_jobs=n_jobs)(delayed(fetch_path)(path) for path in paths)


def combinate(*functions):
    def combinator(*args, **kwargs):
        for f in functions:
            # NOTE no error handling is needed here
            # in no cases should the construction of
            # the python object version of a path fail
            # all failures should happen _after_ construction
            # the way we have implemented this they fail when
            # the data attribute is accessed
            obj = f(*args, **kwargs)
            if isinstance(obj, GeneratorType):
                yield from obj
                # FIXME last one wins, vs yield tuple vs ...?
                # FIXME manifests are completely broken for this
            else:
                yield obj

    return combinator


def multiple(func, merge=None):
    """ combine multiple results """
    def express(*args, **kwargs):
        vs = tuple(func(*args, **kwargs))
        if merge is not None:
            yield merge(vs)
        else:
            yield vs

    return express


def early_failure(path, error, dataset_blob=None):
    # these are the 9999 5555 and 4444 errors
    # TODO match the minimal schema reqs as
    # we did in pipelines
    if dataset_blob is None:
        cache = path.cache
        return {'id': cache.id,
                'meta': {'uri_api': cache.uri_api,
                        'uri_human': cache.uri_human,},
                #'status': 'early_failure',  # XXX note that status is not requried
                # if we cannot compute it, but if there are inputs there should be
                # a status
                'errors': [error],  # FIXME format errro
        }

    else:
        if 'errors' not in datset_blob:
            dataset_blob['errors'] = []

        datset_blob['errors'].append(error)
        return dataset_blob


class DataWrapper:
    # sigh patterns are stupid, move this to elsewhere so it doesn't taint everything
    def __init__(self, data):
        self.data = data


#+end_src
*** _*Test*_
#+begin_src python :tangle ../test/simple/test_utils.py
from sparcur.simple.utils import pipe_main

def test_pipe_main():
    def main(id=None, project_path=None, **kwargs):
        print(id, project_path, kwargs)

    pipe_main(main, argv=['sparcur-simple'])
#+end_src
* Code :noexport:
See also https://docs.racket-lang.org/graphviz/index.html =raco pkg install racket-graphviz=
for more direct mapping of graphviz functionality but one that is also way more verbose.
#+name: racket-graph-helper
#+header: :prologue "#lang racket/base"
#+begin_src racket :lang racket/base :exports none :tangle ./y-u-no-compile-from-buffer.rkt :tangle no
(require graph ; rack pkg install graph
         (only-in racket/string
                  string-trim
                  string-replace)
         (for-syntax racket/base
                     syntax/parse))

(define-for-syntax (list-to-pairs l)
  (for/list ([a l] [b (cdr l)]) (list a b)))

(define-syntax (dag-notation stx)
  (syntax-parse stx
    #:datum-literals (->)
    [(_ (~seq left (~seq -> right) ...) ...)
     #:with (pairs ...) (datum->syntax this-syntax (apply append (map list-to-pairs (syntax->datum #'((left right ...) ...)))))
     #'(unweighted-graph/directed (quote (pairs ...)))]))

(define (subgraph->graphviz subgraph->hash)
  (let ([members (for/list ([(k v) (in-hash (subgraph->hash))] #:when v) k)]
        [label (string-replace (string-trim (symbol->string (object-name subgraph->hash)) "->hash")
                               #rx"[-_?>]"
                               "_")])
    (string-append (format "subgraph cluster_~a" label)
                   ; FIXME this won't quite work because we need to know
                   ; the ids to which the nodes were assigned :/
                   )
    ))

(define (graphviz-subgraphs graph #:subgraphs [subgraph->hash-functions '()])
  "wrap graphviz since it is too simple for our needs at the moment
subgraphs should be specified using vertext properties or edge properties"
  ;; XXX really more clusters
  (define s (graphviz graph))
  (let* ([sl (string-length s)]
         [split-at (- sl 2)]
         [start (substring s 0 split-at)]
         [end (substring s split-at sl)]
         [extra (map subgraph->graphviz subgraph->hash-functions)])
         (apply string-append `(,start ,@extra ,end))))

(module+ test
  (require racket/pretty)
  (define g (dag-notation a -> b -> c
                          b -> d -> e -> f))
  (pretty-print g)
  (graphviz g)

  (define-vertex-property g my-subgraph)
  (for-each (λ (v) (my-subgraph-set! v #t)) '(b c d))

  (define sgh (list my-subgraph->hash))
  (graphviz-subgraphs g #:subgraphs sgh)
)

(module+ test
  ; TODO explor possibility of using -^ or -> ^ or | syntax
  ; to point up to the most recent definition chain containing
  ; the start of the chain in question and having one more
  ; elment than the current chain

  #;
  (dag-notation
   ; for example
   a -> b -> c -> d
        b -> q |
        b -> p |

   ; expands to
   a -> b -> c -> d
        b -> q -> d
        b -> p -> d)

  ; in theory this notation could also be used in reverse, but I'm worried about
  ; accidental hard to debug errors if a line accidentally ends with an arrow

  #;
  (dag-notation
   ; clearly confusing
   a -> b -> c -> d
   | -> d
        | -> e
   ; this would probably read as
   a -> b -> c -> d
   a -> d
   a -> e
   ; hrm
   a -> b -> c -> d
             a |
   | e ; not sure if I like this pretty sure I dont ...
   )
  )
#+end_src
