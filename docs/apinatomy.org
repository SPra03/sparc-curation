# -*- orgstrap-cypher: sha256; orgstrap-norm-func-name: orgstrap-norm-func--prp-1\.1; orgstrap-block-checksum: 66bb730f2134186f319eb0cbd86eb31b89a7703523eb53c5f5182fe75ea2f3bd; -*-
#+title: ApiNATOMY model RDF export and deployment
# [[orgstrap][jump to the orgstrap block for this file]]
#+options: num:nil
#+startup: showall
#+property: header-args:elisp :lexical yes
#+property: header-args :eval no-export

# [[file:./apinatomy.pdf]]
# [[file:./apinatomy.html]]

* Using this file :noexport:
You can also tangle this file to produce [[file:../export-apinatomy-tests]]
and then run that file from the working directory of this sparc-curation repo.
#+begin_src bash
emacs --batch \
      --load org \
      --load ob-shell \
      --load ob-python \
      --eval '(org-babel-tangle-file "./docs/apinatomy.org")'

./export-apinatomy-tests
#+end_src
* ApiNATOMY to RDF/OWL2 conversion
:PROPERTIES:
:visibility: folded
:END:
** Basic strategy
JSON -> RDF -> OWL2 \\
Conversion from json to rdf should have few if any changes in semantics. \\
Conversion from a direct rdf translation to OWL2 is where the semantic \\
translation of ApiNATOMY structures into OWL2 constructs will happen.
* Server setup
:PROPERTIES:
:visibility: folded
:END:
On the ontology host (read, ttl host, not SciGraph host) you will need the following.
#+begin_src bash :dir /ssh:host-apinat-ttl|sudo:host-apinat-ttl: :eval never
mkdir /var/www/sparc/ApiANTOMY
mkdir /var/www/sparc/ApiANTOMY/archive
mkdir /var/www/sparc/ApiANTOMY/archive/manual
mkdir /var/www/sparc/ApiANTOMY/ontologies
chown -R nginx:nginx /var/www/sparc/ApiANTOMY
#+end_src
* SciGraph pipeline
** Parallel build
#+begin_src bash
function apinat-build () {
    # add this line if you need to get a traceback on error
    # --funcall toggle-debug-on-error \
    emacs --quick --batch \
    --load ~/git/orgstrap/orgstrap-batch-helper.el \
    --visit ~/git/sparc-curation/docs/apinatomy.org \
    -- ${@}
}
#+end_src

#+begin_src bash
export -f apinat-build
pushd ~/git/apinatomy-models
ls -d *{-,bron}* | \
xargs -P10 -r -I {} \
      bash -c 'apinat-build --repo $(pwd) --secrets ~/path/to/secrets.sxpr --model-id ${@}' _ {}

# only add models where the json has changed since the xlsx is opaque
git add -u $(git status --porcelain | grep json | grep M | cut -d' ' -f3 | cut -d'/' -f1)
#+end_src

** Export to ttl
#+link: r-apin-mod git:79316499d7987f73a56ce2bc54d07afe91886cd1:

# these link abbreviations should be in the file themselves, or materialized from a common source
# in a way that can be synced, or actually it is probably ok to put them in a setup/startup file
# as long as orgstrap tells you how to get that file
# the gsl local index should not be here in the file, but the local path names can and should be
#+git-share-local: git:79316499d7987f73a56ce2bc54d07afe91886cd1:HEAD: file:~/git/apinatomy-models/
r-apin-mod:HEAD:{model-id}/
#+link: gsx https://docs.google.com/spreadsheets/d/%s/export?format=xlsx
YEAH it DOES support arbitrary locations, with the ~%s~ implicitly at the tail by default

gsx:google-sheet-id

# org
# org-set-regexps-and-options

#+name: flow-to-ttl
#+begin_src elisp :results none
(defvar-local apinat-converter-path nil
  "full path to javascript apinatomy converter")

(defvar-local apinat-converter-debug nil
  "dynamic variable used to control debug behavior")

(defun-local ex-do (&rest nothing)
  "Executor do. Eats the input since it is to be run by the executor."
  ;; maybe use this to resolve the ex-come-from flows?
  ;; while loop would be annoying here
  (yes-or-no-p "Step done? "))

(defun-local ex-label (label) ;; call/cc, prompt, or cl condition handling is likely the abstraction we would want
  "Target acquired!")

(defun-local ex-come-from (label)
  ;; detangling what is going on when using this, there is an implicit assertion
  ;; that a linear set of `ex-do' steps result in the same final state as a series
  ;; of sequential calls, the reason to use come-from is that the conductor program
  ;; can return and go on to the next step as expected (though a post condition)
  ;; measurement ala a contract needs to be confirmed, the conductor continues
  ;; along to the next step, and the extracted and compiled `ex-do' (which is really
  ;; a macro) command uses the come-from to stich the dependency tree back together
  ;; for the real world steps that are most efficient, the problem with this is that
  ;; you have to prove that the ordering for both graphs is compatible, namely that
  ;; for a particular set of sequential calls that there is only a single come-from
  ;; per line, otherwise you have to figure out what it means to have 20 things that
  ;; can all in princilple happen after the completion of a step, which is possible
  ;; but would need a solution
  "AAAAAAAAA!")

(defvar *apinatomy-model-repository* "~/git/apinatomy-models/")

(defmacro apinat--with-model (model-id &rest body)
  (declare (indent defun))
  `(let ((default-directory (expand-file-name (symbol-name ,model-id) *apinatomy-model-repository*))
         (xlsx    (format "source/%s.xlsx"    model-id))
         (json    (format "source/%s.json"    model-id))
         (jsonld  (format "derived/%s.jsonld" model-id))
         (ttl     (format "derived/%s.ttl"    model-id)))
     ,@body))

(defun-local flow-to-ttl (model-id)
  ;; probably need 1 more level of indirection to handle cases where the model-id
  ;; will be put inside the models/ folder which should probably happen sooner rather than later
  ;;(let ((default-directory (expand-file-name model-id (git-share-local r-apin-mod:HEAD:))) ; TODO ))
  '
  (apinat--with-model model-id
    ;; TODO make sure the directories exist
    ;; TODO resume from previous step on failure <- this is critical
    ;; check out `file-newer-than-file-p' as a reasonable approach ala make
    ;; the only issue is how to prevent the xlsx retrieval from notching it up
    ;; every single time, maybe we can compare checksums on the xlsx file?
    ;; TODO push the model id further down the call chain since input
    ;; and output paths are defined by convention
    (funcall (checksum-or #'-mx->)  model-id   xlsx) ; source/{model-id}.xlsx
    ;;; FIXME TODO message about opening the open physiology viewer
    (-xjl-> xlsx json jsonld)
    ;;(funcall (out-or      #'-xj->)  xlsx       json) ; source/{model-id}.xlsx source/{model-id}.json
    ;;(funcall (out-or      #'-jl->)  json     jsonld) ; source/{model-id}.json derived/{model-id}.jsonld
    (funcall (out-or      #'-lt->)  jsonld      ttl) ; derived/{model-id}.jsonld derived/{model-id}.ttl
                      )

  (apinat--with-model model-id
                      (-mxjl-> model-id xlsx json jsonld)
                      (-lt-> jsonld ttl)))

(defun-local checksum-or (fun)
  (lambda (thing path-out)
    ;;(message "dd: %s" default-directory)
    (if (file-exists-p path-out)
        (let ((path-temp (let ((temporary-file-directory default-directory))
                           ;; keep the temp nearby rather than where ever the tfd is
                           (make-temp-file (concat path-out "._maybe_new_")))))
          (unwind-protect
              (progn
                (funcall fun thing path-temp)
                (let ((checksum-new (securl-path-checksum path-temp))
                      (checksum-old (securl-path-checksum path-out)))
                  (unless (string= checksum-new checksum-old)
                    (rename-file path-temp path-out t))))
            (when (file-exists-p path-temp)
              (delete-file path-temp))))
      (funcall fun thing path-out))))

(defun-local out-or (fun)
  (lambda (path-in path-out)
    (when (and (file-exists-p path-in)
               (or (not (file-exists-p path-out))
                   (and (file-exists-p path-out)
                        (file-newer-than-file-p path-in path-out))))
      (funcall fun path-in path-out))))

;; model id -> some path

(defun-local -mt-> (model-id path-ttl) "`model-id' to `path-ttl'")
(defun-local -ml-> (model-id path-jsonld) "`model-id' to `path-jsonld'")
(defun-local -mxjl-> (model-id path-xlsx path-json path-jsonld)
  "`model-id' to everything except the ttl"
  (let ((path-xlsx (concat default-directory "/" path-xlsx))
        (path-json (concat default-directory "/" path-json))
        (path-jsonld (concat default-directory "/" path-jsonld))
        (path-temp (make-temp-file "apinat-conversion" 'directory)))
    (unwind-protect
        (let* ((default-directory path-temp)
               (google-sheet-id (symbol-name (-ms-> model-id)))
               (path-internal (concat path-temp "/build")) ; should not exist to avoid date suffix
               (_ (run-command apinat-converter-path
                               "-f" "id"
                               "-t" "xlsx"
                               "-t" "json"
                               "-t" "json-flattened"
                               "-i" google-sheet-id
                               "-o" path-internal)))
          (cl-loop for path in (list path-xlsx path-json path-jsonld) do
                   (let ((parent (file-name-directory path)))
                     (unless (file-directory-p parent)
                       (make-directory parent t))))
          (rename-file (concat path-internal "/" "model.xlsx") path-xlsx t)
          (rename-file (concat path-internal "/" "model.json") path-json t)
          (rename-file (concat path-internal "/" "model-flattened.jsonld") path-jsonld t))
      (unless apinat-converter-debug
        (delete-directory path-temp 'recursive)))))

(defun-local -mj-> (model-id path-json) "`model-id' to `path-json'")

;; intermediate steps for model id

(defun-local -m-lt-> (model-id)
  (apinat--with-model model-id
                      (funcall (out-or #'-lt->) jsonld ttl)))

(defun-local -m-x-> (model-id)
  (apinat--with-model model-id
                      (-mx-> model-id xlsx)))

(defun-local -ms-> (model-id)
  (oa-path :google :sheets (if (keywordp model-id)
                               model-id
                             (intern (format ":%s" model-id)))))

(defun-local -mx-> (model-id path-xlsx)
  ;; automated
  (let* ((google-sheet-id (-ms-> model-id))
         (url (format "https://docs.google.com/spreadsheets/d/%s/export?format=xlsx" google-sheet-id)))
    ' ; it probably makes more sense to implement stuff like this using the condition system?
    ;; in terms of flow control for a DAG you try to do the thing,
    ;; stop at your first error and then go do the dependency? but in
    ;; reality there is often an explicit step where all checks must
    ;; pass before the whole process can continue because of some time
    ;; constraint or similar
    (ex-do (message "Make sure that the permissions are set correctly on %s" url))
    ;; NOTE `url-copy-file' cannot detect login redirects correclty
    ;; google sends a 307 for the download if everything is going to work
    ;; in curl it sends a 302 but never something in the 400 range
    ;; ideally we would be able to (run-command "mimetype" path-xlsx)
    ;; but that requires that users have the mimetype command avaiable
    (url-copy-file url path-xlsx t)))

(defun-local -xj-> (path-xlsx path-json)
  "This is currently a manual step."
  (let (;(open-physiology-viewer "file:///home/tom/git/open-physiology-viewer/dist/test-app/index.html")
        (open-physiology-viewer "https://open-physiology-viewer.surge.sh/"))
    ;; TODO conditional open only if not already
    ;;(browse-url open-physiology-viewer)
    ;;(run-command "google-chrome-unstable" open-physiology-viewer)
    (ex-do (message "open file (left top folder) to upload to viewer from %s" path-xlsx)
           (message "save file (left bottom floppy) to download from viewer to %s" path-json)
           (ex-label 'viewer-after-open))))

(defun-local -jl-> (path-json path-jsonld)
  "Currently a manual step."
  (ex-do (ex-come-from 'viewer-after-open) ; This is amazing.
         ;; Allows decoupling of functional spec from the actual execution in the real world.
         ;; As a bonus we get to use my all time favorite control flow structure.
         (message "export flattened json-ld (right 2nd from bot white doc) to download from viewer to %s"
                  path-jsonld)))

(defun-local -xjl-> (path-xlsx path-json path-jsonld)
  ;; yay automated NOTE requires nodejs and open-physiology-viewer
  (let ((path-xlsx (concat default-directory "/" path-xlsx))
        (path-json (concat default-directory "/" path-json))
        (path-jsonld (concat default-directory "/" path-jsonld))
        (path-temp (make-temp-file "apinat-conversion" 'directory)))
    (unwind-protect
        (let* ((default-directory path-temp)
               (_ (run-command apinat-converter-path "-m" "xlsx" "-i" path-xlsx))
               (output-dir (car (directory-files default-directory nil "converted-*"))))
          ;; '("model-flattened.jsonLD" "model-generated.json" "model.json" "model.jsonLD")
          (rename-file (concat output-dir "/" "model.json") path-json t)
          (rename-file (concat output-dir "/" "model-flattened.jsonLD") path-jsonld t))
      (delete-directory path-temp 'recursive))))

(defun-local -lt-> (path-jsonld path-ttl)
  ;; automated
  (run-command (or (executable-find "pypy3")
                   (executable-find "python"))
               "-m" "sparcur.cli" "apinat" path-jsonld path-ttl))
#+end_src

#+name: all-ttl-models
#+begin_src elisp :results none
(defun-local update-models (model-ids) ; vs &rest model-ids
  ;; FIXME mapcar is inadequate for handling parallel processes that
  ;; might have `ex-do' parts
  (mapcar #'flow-to-ttl model-ids))

(defun-local all-models ()
  ;;(let ((default-directory (git-share-local r-apin-mod:HEAD:)) ; TODO ))
  (let ((default-directory (expand-file-name *apinatomy-model-repository*)))
    ;; you could use something like model-repository but then you have to make
    ;; a bunch of concatentations, better just to switch the default directory
    ;; so that the context deals with alignment between name and local referent
    (cl-remove-if (lambda (p) (or (not (file-directory-p p)) (string-prefix-p "." p)))
                  (directory-files default-directory))))

(defun apinat--ttl-newer (model-id)
  (apinat--with-model model-id
    (let ((mtimes
           (mapcar (lambda (p)
                     (string-to-number
                      (format-time-string
                       "%s"
                       (file-attribute-modification-time (file-attributes p)))))
                   (list ttl xlsx))))
      (message "%S" mtimes)
      (and (file-exists-p ttl)
           (apply #'> mtimes)))))

(defun-local filter-recent-models (model-ids)
  (cl-remove-if #'apinat--ttl-newer model-ids))

(defun-local update-all-models (&optional skip-recent)
  (update-models (if skip-recent
                     (filter-recent-models (mapcar #'intern (all-models)))
                   (mapcar #'intern (all-models)))))
#+end_src

#+begin_src elisp
;; FIXME do fetch all in one batch so we don't have
;; to wait for the ttl export between each model
(update-all-models t)
' ; or pick your own models
(update-models '(vagus-nerve))
' ; jsonld -> ttl conversion
(-m-lt-> 'vagus-nerve)
#+end_src

if error clone the repo
#+begin_src sh
pushd ~/git
git clone https://github.com/open-physiology/apinatomy-models.git
#+end_src
if model-id error then we need to set the model ids in secrets but in
reality need to overwrite the defniition of ~-ms->~ is easier right
now

missing derived folders
#+begin_src powershell
pushd ~/git/apinatomy-models/
New-Item -Path * -Name derived -ItemType "directory"
#+end_src

#+begin_src bash
pushd ~/git/apinatomy-models/
find -maxdepth 1 -type d -not -path '*.git*' -not -path '.' -exec mkdir {}/derived \;
#+end_src
** ttl deprecated :noexport:
Until this is fully automated you need to obtain the flattened jsonld
export of any files to be converted to ttl.

Make sure SciGraph services and InterLex are accessible for OntTerm.

Run this block in emacs with =C-c C-c= or tangle and run with the block below
#+name: apinat-export
#+header: :shebang "#!/usr/bin/env bash" :tangle-mode (identity #o0755)
#+begin_src bash :dir ../ :tangle ../export-apinatomy-tests :async
spc apinat bolser-lewis.jsonld bolser-lewis.ttl
spc apinat keast-bladder.jsonld keast-bladder.ttl
spc apinat bronchomotor.jsonld bronchomotor.ttl
#+end_src
# note have to export to working dir not ../bin/ because
# there is no test folder inside of bin and python can't
# look backward up the folder hierarchy to find it

** Deploy ttl
After running the ttl export define the functions in
ref:deploy-ontology-file and then run ~apinat-deploy-from-ttl
bronchomotor.ttl~. NOTE Both functions need to be defined.

The current command to deploy all is.
#+begin_src bash
for f in $(ls */derived/*.ttl); do apinat-deploy-from-ttl $f; done
#+end_src

Alternately use the following to deploy specific models.
#+begin_src bash
apinat-deploy-ttls $(git diff --name-only HEAD~1..HEAD | cut -d'/' -f 1 | sort -u)
#+end_src

If you add a new model you will need to update the imports in
https://cassava.ucsd.edu/ApiNATOMY/ontologies/sparc-data.ttl.
The update process should be automated as part of the workflows
described here. See also [[file:./../resources/scigraph/ontologies-sparc-data.yaml]].

# [[tramp:/ssh:cassava|sudo:cassava:/var/www/sparc/ApiNATOMY/ontologies/sparc-data.ttl]]

# FIXME it should be possible to implement this whole process
# using OntResIriWrite or something like that
# read the header, lookup the uri -> server file system path
# write the version iri if it doesn exist (otherwise error)
# and symlink it to the remote, I don't have an implementation
# of RemoteUnixPath that could use something like sftp to
# allow direct execution of file operations on a remote path
# from a local python representation of that class so it is
# too big to bite off right now

#+name: deploy-ontology-file
#+begin_src bash
function apinat-remote-operations () {
    local PATH_SOURCE="${1}"
    local PATH_TARGET="${2}"
    local PATH_LINK="${3}"
    local FILE_NAME_TTL=$(basename -- "${PATH_TTL}")
    local DIR_LINK="$(dirname "${PATH_LINK}")"
    local LINK_TARGET="$(realpath -m --relative-to="${DIR_LINK}" "${PATH_TARGET}")"
    mkdir -p "$(dirname "${PATH_TARGET}")"
    chown nginx:nginx "${PATH_SOURCE}"
    mv "${PATH_SOURCE}" "${PATH_TARGET}"
    unlink "${PATH_LINK}"
    ln -s "${LINK_TARGET}" "${PATH_LINK}"
}

function apinat-deploy-from-ttl () {
    # TODO loop over positional argument paths, but retain a single ssh command
    local PATH_TTL="${1}"  # FIXME careful with this, never allow a user to set the source path
    local DATE=$(date +%s)  # FIXME source from the ontology directly? better to spend time implementing OntResIriWrite
    local HOST_APINAT_ONTOLOGY=cassava
    local FILE_NAME_TTL=$(basename -- "${PATH_TTL}")
    local NAME_TTL="${FILE_NAME_TTL%.*}"
    local PATH_REMOTE_TARGET_BASE=/var/www/sparc/ApiNATOMY/ontologies/
    local VERSION_PATH="${NAME_TTL}/${DATE}/${FILE_NAME_TTL}"
    local PATH_REMOTE_SOURCE="/tmp/${FILE_NAME_TTL}"
    local PATH_REMOTE_TARGET="${PATH_REMOTE_TARGET_BASE}${VERSION_PATH}"
    local PATH_REMOTE_LINK="${PATH_REMOTE_TARGET_BASE}${FILE_NAME_TTL}"

    # FIXME also notify host for sudo
    local SUDO_OR_SU='$(command -v sudo 1>& 2 && echo sudo ${0} -c || { echo For su on ${HOSTNAME} 1>& 2; echo su -c; })'

    # TODO ensure that apinat-remote-operations is defined
    rsync --rsh ssh "${PATH_TTL}" ${HOST_APINAT_ONTOLOGY}:"${PATH_REMOTE_SOURCE}"
    ssh -t ${HOST_APINAT_ONTOLOGY} "${SUDO_OR_SU} '$(typeset -f apinat-remote-operations); apinat-remote-operations \
\"${PATH_REMOTE_SOURCE}\" \
\"${PATH_REMOTE_TARGET}\" \
\"${PATH_REMOTE_LINK}\"'"
}

function apinat-deploy-ttls () {
    # TODO do it in batch, derive the timesamps correctly etc.
    for id in $@; do
        apinat-deploy-from-ttl "${id}/derived/${id}.ttl"
    done
}
#+end_src

Check [[https://cassava.ucsd.edu/ApiNATOMY/ontologies/]] for success if needed.
# [[tramp:/ssh:cassava|sudo:cassava:/var/www/sparc/ApiNATOMY/ontologies/sparc-data.ttl]]

#+begin_src bash
spc report changes \
--ttl-file https://cassava.ucsd.edu/ApiNATOMY/ontologies/keast-bladder/1620348301/keast-bladder.ttl \
--ttl-compare https://cassava.ucsd.edu/ApiNATOMY/ontologies/keast-bladder/1617055182/keast-bladder.ttl
#+end_src
** Load and deploy graph
Then run
[[file:~/git/pyontutils/nifstd/scigraph/README.org::run-load-deploy-graph-sparc-data][run-load-deploy-graph-sparc-data]]
to load and deploy in one shot.

An example run is
#+begin_src bash
~/git/pyontutils/nifstd/scigraph/bin/run-load-graph-sparc-data
~/git/pyontutils/nifstd/scigraph/bin/run-deploy-graph-sparc-data
#+end_src
# TODO consider ob-screen ... for cases like this
# where we aren't really writing bash so much as just
# running commands
** Review query output
[[http://ontology.neuinfo.org/trees/sparc/dynamic/demos/apinat/somas][All somas]]
[[http://ontology.neuinfo.org/trees/sparc/dynamic/demos/apinat/soma-processes][Soma processes]]
[[http://ontology.neuinfo.org/trees/sparc/simple/dynamic/demos/apinat/soma-processes][Soma processes simple]]
* Dynamic cypher queries
:PROPERTIES:
:visibility: folded
:END:
NOTE: this section contains temporary instructions.
This should really be done on a development instance of data services.
Sometimes it is faster to edit [[tramp:/ssh:aws-scigraph-data-scigraph:services.yaml]] directly.
Use the following command to restart services to load the updated dynamic queries.
#+begin_src bash :results none
ssh aws-scigraph-data sudo systemctl restart scigraph
#+end_src
When you have a query working as desired add it or update it in
[[file:../resources/scigraph/cypher-resources.yaml][cypher resources]].
# TODO need that local/remote git link ...
See also [[file:../../pyontutils/nifstd/scigraph/README.org::#sparc-data-services-build-deploy][data services build and deploy]].
* Add new ApiNATOMY model to SciGraph load
Edit [[file:../resources/scigraph/sparc-data.ttl][sparc-data.ttl]] and
add a new line to the second =owl:import= statement.
* ApiNATOMY model server specification
:PROPERTIES:
:visibility: folded
:END:
# file is in pyontutils/nifstd/resolver
** Intro
While an ApiNATOMY server has been on the roadmap for some time, there have not been
clear requirements and use cases to drive the development in a way that is productive.
As the conversion of ApiNATOMY models to RDF has progressed, some of the requirements
and use cases have presented themselves and helped to solidify a set of initial use cases.
The need to integrate knowledge represented in ApiNATOMY into the larger linked data space
provides some initial requirements which are the that the server be able to provide persistent
and resolvable identifiers for ApiNATOMY models, and that it be able to provide high granularity
access to the version history of these models. In addition, we are ultimately aiming for
the server to be able to automatically convert input models or spreadsheets into generated
models and resource maps. We have mapped out three phases for arriving at this end goal.
The first phase is to be able to resolve input models, the second is to be able to upload
and link the generated model and resource map and track which input model they came from.
These two will address our primary short-term needs.

To accomplish this, the plan is to use git (via GitHub) as the primary datastore for the models.
This will allow us to leverage the significant existing infrastructure around GitHub for version
control, collaboration, review, content hosting, and backup. In front of this there will be a
server that provides resolvable persistent identifiers for ApiNATOMY models so that the identifiers
appearing in the linked data graphs will be resolvable and interoperable with the rest of the
NIF-Ontology search and discovery tooling.

In the future as part of the third phase we can work towards automating the conversion of input models,
and it might also be possible to have the server automatically convert and serve the RDF version of the
models as well.

A brief outline of the initial requirements needed to meet the needs of the RDF conversion pipeline
are documented below.
** Architecture diagram
[[file:./images/apinatomy-server-diagram.png]]
Legend.
| Solid lines         | initial  |
| Dashed lines        | soon     |
| Dotted lines        | later    |
| Dashed dotted lines | dataflow |
** https by default
** url structure
*** apinatomy.org
alternately https://uri.apinatomy.org
**** /uris/models/{model-id}.{ext}
how to deal with json/ttl and model, generated, map
**** /uris/models/{model-id}/ids/{local-id}
**** /uris/readable/{string}
**** /uris/elements/{string}
** transformed models/copies need to be able to point back to the exact commit
for deposition on blackfynn, export to scigraph, etc.
the source model hash needs to be separat
** Serve the JSONLD context
** return authoring metadata
** store the source model
** have endpoint for resource-map and generated
** overlap with loading in the client
*** load all formats from local
*** google sheets import
*** load from a url
* Reporting
#+begin_src python :epilogue "return main()" :exports both
import json
import augpathlib as aug
from pyontutils.core import OntGraph
from pyontutils.namespaces import rdf, owl


def path_json(string):
    with open(string, 'rt') as f:
        return json.load(f)


def main():
    graph = OntGraph()
    apinat_models = aug.RepoPath('~/git/apinatomy-models').expanduser()
    [graph.parse(f) for f in apinat_models.rglob('*.ttl')]
    # rdf
    n_trip = len(graph)
    n_class = len(set(graph[:rdf.type:owl.Class]))
    n_ind = len(set(graph[:rdf.type:owl.NamedIndividual]))
    # json
    js = [path_json(p) for p in apinat_models.rglob('*.json')]
    keys = ('publications', 'nodes', 'links', 'lyphs', 'materials', 'chains', 'groups')
    n_obj = sum([sum([len(j[k])
                      if k in j else 0 for k in keys])
                 for j in js])
    n_pair = sum([sum([sum([len(o) for o in j[k]])
                       if k in j else 0 for k in keys])
                  for j in js])
    print(f'''rdf
trip:  {n_trip}
class: {n_class}
ind:   {n_ind}

json
obj:   {n_obj}
obj:   {n_pair}''')
    return [['Type', 'Authored', 'Expanded'],
            ['Individual', n_obj, n_ind],
            ['Statement', n_pair, n_trip],
            ['owl:Class', 'n/a', n_class],]
#+end_src

#+RESULTS:
| Type       | Authored | Expanded |
|------------+----------+----------|
| Individual |     1714 |    25940 |
| Statement  |     8274 |   318378 |
| owl:Class  |      n/a |      395 |

* Bootstrap :noexport:

#+name: orgstrap
#+begin_src elisp :results none :lexical yes :noweb yes
;; TODO suppress the welcome screen
;; TODO automatically run flow-to-ttl probably

;;; load remote code

(unless (featurep 'reval)
  (defvar reval-cache-directory (concat user-emacs-directory "reval/cache/"))
  (defun reval-minimal (cypher checksum path-or-url &rest alternates)
    "Simplified and compact implementation of reval."
    (let* (done (o url-handler-mode) (csn (symbol-name checksum))
                (cache-path (concat reval-cache-directory (substring csn 0 2) "/" csn
                                    "-" (file-name-nondirectory path-or-url))))
      (url-handler-mode)
      (unwind-protect
          (cl-loop for path-or-url in (cons cache-path (cons path-or-url alternates))
                   do (when (file-exists-p path-or-url)
                        (let* ((buffer (find-file-noselect path-or-url))
                               (buffer-checksum (intern (secure-hash cypher buffer))))
                          (if (eq buffer-checksum checksum)
                              (progn
                                (unless (string= path-or-url cache-path)
                                  (let ((parent-path (file-name-directory cache-path))
                                        make-backup-files)
                                    (unless (file-directory-p parent-path)
                                      (make-directory parent-path t))
                                    (with-current-buffer buffer
                                      (write-file cache-path))))
                                (eval-buffer buffer)
                                (setq done t))
                            (kill-buffer buffer) ; kill so cannot accidentally evaled
                            (error "reval: checksum mismatch! %s" path-or-url))))
                   until done)
        (unless o
          (url-handler-mode 0)))))
  (defalias 'reval #'reval-minimal)
  (reval 'sha256 'f978168b5c0fc0ce43f69c748847e693acc545df9a3ff1d9def57bdb1fc63c4a
         "https://raw.githubusercontent.com/tgbugs/orgstrap/649fd0cdcb831dcd840c66ee324005165ce970ca/reval.el"))

(let ((ghost "https://raw.githubusercontent.com/tgbugs/orgstrap/"))
  (unless (featurep 'ow)
    (reval 'sha256 'a90b12c386d60882cadeb6b6557f7eb05378bfcf94f68f7f8512a9edfeb34d6c
           (concat ghost "98350dc97b6a079d35c94b1798501a62cbbdf176" "/ow.el"))))

(unless (fboundp 'run-command)
  ;; ow.el doesn't set the alias because it is doubles as a real package
  (defalias 'run-command #'ow-run-command))

;; local function definitions

<<flow-to-ttl>>

<<all-ttl-models>>

<<temp-parse-args>>

(unless (fboundp #'oa-path)
  <<temp-orthauth>>
  )

(let ((args-left command-line-args-left))
  (setq command-line-args-left nil) ; prevent emacs from trying to open our other arguments
  (when (and noninteractive ; have to use this since --batch is stripped
             (string= (car args-left) "--"))
    (let* ((command-line-args-left args-left)
           (args-alist (parse-args
                        (:repo "~/git/apinatomy-models/")
                        (:converter "apinat-converter")
                        (:model-id nil)
                        (:secrets nil)
                        (:debug))))
      (let ((*apinatomy-model-repository* (cdr (assq 'repo args-alist)))
            (apinat-converter-path (cdr (assq 'converter args-alist)))
            (apinat-converter-debug (cdr (assq 'debug args-alist)))
            (model-id (intern (cdr (assq 'model-id args-alist))))
            (oa-secrets (cdr (assq 'secrets args-alist))))
        (message "updating %s" model-id)
        (update-models (list model-id))))))
#+end_src

Temporary extract of functionality needed to manage =model-id= to =google-sheet-id= mapping.
#+name: temp-orthauth
#+begin_src elisp :results none
(defvar oa-secrets nil "path to secrets file")

(defun oa--resolve-path (plist elements)
  "recursively `cl-getf' in order keywords from ELEMENTS in nested plists inside PLIST"
  (if elements
      (oa--resolve-path (cl-getf plist (car elements)) (cdr elements))
    plist))

(defun oa-read (path)
  "read the first sexpression in the file at path"
  (with-temp-buffer
    (insert-file-contents path)
    (read (buffer-string))))

(defun oa-path (&rest elements)
  "Retrieve value at nested path defined by keywords provided in ELEMENTS in `oa-secrets'"
  (let ((plist (oa-read oa-secrets)))
    (oa--resolve-path plist elements)))
#+end_src

Temporary extract for simple command line argument parsing.
#+name: temp-parse-args
#+begin_src elisp :results none
;;; from git share parse-args

(defun saner-string-to-number (string &optional base)
  "vanilla `string-to-number' has a degenerate case with \"0\""
  (let ((maybe-zero (string-to-number string base)))
    (if (= maybe-zero 0)
        (if (string= maybe-zero "0")
            0
          (error "%S is not a number!" string))
      maybe-zero)))

(defun norm-arg (arg)
  (let ((int (ignore-errors (saner-string-to-number arg))))
    (if int int arg)))

(defun keyword-name (keyword)
  (unless (keywordp keyword)
    (error "%s is not a keyword! %s" keyword (type-of keyword)))
  (substring (symbol-name keyword) 1))

(defun process-keyword (element)
  (unless (listp element)
    (error "%s not a list! %s" element (type-of element)))
  (let* ((kw (car element))
         ;;(sl (string-downcase (symbol-name kw)))
         (sl (keyword-name kw)) ; emacs is case preserving
         (assign (cdr element))  ; FIXME default? FIXME XXX empty vs explicit nil
         ;;(real-assign (if assign (car assign) (intern (symbol-name kw))))
         (real-assign (intern (keyword-name kw)))
         (default (if assign (car assign) assign)) ; FIXME
         (p (if assign
                `(progn (setf ,real-assign (norm-arg (cadr args)))
                        ;; equivalent of bash shift shift
                        (setf args (cddr args)))
                `(progn (setf ,real-assign t)
                        ;; equivalent of bash shift
                        (setf args (cdr args))))))
    (list `(,real-assign ,default)  ; default
          `(,(intern (format "--%s" sl)) ,p)  ; case
          `(cons ',real-assign ,real-assign))))

(defmacro parse-args (&rest keywords)
  "(parse-args (:port port) (:pid pid) (:flag))

   NOTE if the default value if a kwarg is nil rather than
   empty i.e. (:asdf nil) vs (:asdf) the form with nil will
   not fail but will be nil unless some value is provided
   AND it will eat the next kwarg this is probably a misdesign"
  (cl-destructuring-bind (defaults cases returns)
      (apply #'cl-mapcar #'list ; `cl-mapcar' required for this to work
             (mapcar #'process-keyword keywords))
    `(let ((args (cdr command-line-args-left))
           ,@defaults)
       (cl-do ()
           ((null args) nil)
         (cl-case (intern (car args))
           ,@cases
           (otherwise (progn (message "unhandled: %s" (car args))
                             (setf args (cdr args))))))
       (list ,@returns))))
#+end_src

** Local Variables :ARCHIVE:

# Local Variables:
# eval: (progn (setq-local orgstrap-min-org-version "8.2.10") (let ((actual (org-version)) (need orgstrap-min-org-version)) (or (fboundp #'orgstrap--confirm-eval) (not need) (string< need actual) (string= need actual) (error "Your Org is too old! %s < %s" actual need))) (defun orgstrap-norm-func--prp-1\.1 (body) (let (print-quoted print-length print-level) (prin1-to-string (read (concat "(progn\n" body "\n)"))))) (unless (boundp 'orgstrap-norm-func) (defvar orgstrap-norm-func orgstrap-norm-func-name)) (defun orgstrap-norm-embd (body) (funcall orgstrap-norm-func body)) (unless (fboundp #'orgstrap-norm) (defalias 'orgstrap-norm #'orgstrap-norm-embd)) (defun orgstrap-org-src-coderef-regexp (_fmt &optional label) (let ((fmt org-coderef-label-format)) (format "\\([:blank:]*\\(%s\\)[:blank:]*\\)$" (replace-regexp-in-string "%s" (if label (regexp-quote label) "\\([-a-zA-Z0-9_][-a-zA-Z0-9_ ]*\\)") (regexp-quote fmt) nil t)))) (unless (fboundp #'org-src-coderef-regexp) (defalias 'org-src-coderef-regexp #'orgstrap-org-src-coderef-regexp)) (defun orgstrap--expand-body (info) (let ((coderef (nth 6 info)) (expand (if (org-babel-noweb-p (nth 2 info) :eval) (org-babel-expand-noweb-references info) (nth 1 info)))) (if (not coderef) expand (replace-regexp-in-string (org-src-coderef-regexp coderef) "" expand nil nil 1)))) (defun orgstrap--confirm-eval-portable (lang _body) (not (and (member lang '("elisp" "emacs-lisp")) (let* ((body (orgstrap--expand-body (org-babel-get-src-block-info))) (body-normalized (orgstrap-norm body)) (content-checksum (intern (secure-hash orgstrap-cypher body-normalized)))) (eq orgstrap-block-checksum content-checksum))))) (defalias 'orgstrap--confirm-eval #'orgstrap--confirm-eval-portable) (let ((ocbe org-confirm-babel-evaluate)) (setq-local orgstrap-norm-func orgstrap-norm-func-name) (setq-local org-confirm-babel-evaluate #'orgstrap--confirm-eval) (unwind-protect (save-excursion (org-babel-goto-named-src-block "orgstrap") (org-babel-execute-src-block)) (setq-local org-confirm-babel-evaluate ocbe) (org-set-startup-visibility))))
# End:
